#!/usr/bin/env python3
"""
Journal Ranking Data Updater - Enhanced Version
Collects journal ranking data from multiple sources using FlareSolverr
"""

import json
import yaml
import requests
import time
import re
import os
import sys
import argparse
from datetime import datetime
import logging
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
import random
from dotenv import load_dotenv

load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# FlareSolverr configuration
FLARESOLVERR_URL = "http://127.0.0.1:8191"
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
]

class FlareSolverrClient:
    """Client for FlareSolverr to bypass anti-bot protection (Enhanced for Wiley)"""
    
    def __init__(self, base_url: str = FLARESOLVERR_URL):
        self.base_url = base_url
        self.session = None
        
    def create_session(self) -> Optional[str]:
        """Create a new FlareSolverr session"""
        try:
            # é”€æ¯æ—§ session ä»¥é˜²æ®‹ç•™
            if self.session:
                self.destroy_session()
                
            session_id = f"journal_session_{int(time.time())}"
            response = requests.post(f"{self.base_url}/v1", json={
                "cmd": "sessions.create",
                "session": session_id,
                # æ˜¾å¼æŒ‡å®šæµè§ˆå™¨å‚æ•°ï¼Œå°è¯•æ¨¡æ‹ŸçœŸå®ç¯å¢ƒ
                "userAgent": random.choice(USER_AGENTS) 
            }, timeout=30)
            
            data = response.json()
            if data.get("status") == "ok":
                self.session = session_id
                logger.info(f"Created FlareSolverr session: {self.session}")
                return self.session
            else:
                logger.error(f"Failed to create session: {data}")
                return None
        except Exception as e:
            logger.error(f"Error creating FlareSolverr session: {e}")
            return None
    
    def get_page(self, url: str) -> Optional[str]:
        """Get page content using FlareSolverr with Retry Logic"""
        # å¢åŠ æœ€å¤§è¶…æ—¶æ—¶é—´åˆ° 3 åˆ†é’Ÿ (180000ms)
        # Wiley çš„äº”ç§’ç›¾æœ‰æ—¶å€™ä¼šå¡å¾ˆä¹…
        max_timeout = 180000 
        
        for attempt in range(2): # å°è¯• 2 æ¬¡
            if not self.session:
                if not self.create_session():
                    return None
            
            try:
                logger.info(f"   ğŸ”„ Requesting page (Attempt {attempt+1}): {url}")
                
                # æ³¨æ„ï¼šPython çš„ requests timeout å¿…é¡»æ¯” FlareSolverr çš„ maxTimeout å¤§
                # è¿™é‡Œè®¾ä¸º 190ç§’ï¼Œç»™ FlareSolverr ç•™å‡º 180ç§’ å¤„ç†æ—¶é—´
                response = requests.post(f"{self.base_url}/v1", json={
                    "cmd": "request.get",
                    "url": url,
                    "maxTimeout": max_timeout,
                    "session": self.session,
                    # åªè¦ HTML ä¸‹è½½å®Œå°±ç®—æˆåŠŸï¼Œä¸éœ€è¦ç­‰æ‰€æœ‰å›¾ç‰‡åŠ è½½å®Œ (networkidle0æœ‰æ—¶ä¼šå¡æ­»)
                    "returnOnlyHtml": True 
                }, timeout=190) 
                
                if response.status_code == 500:
                    logger.warning(f"   âš ï¸ FlareSolverr 500 Error (Timeout?). Destroying session and retrying...")
                    self.destroy_session() # é”€æ¯å½“å‰ sessionï¼Œä¸‹æ¬¡å¾ªç¯ä¼šé‡å»º
                    continue

                response.raise_for_status()
                data = response.json()
                
                if data.get("status") == "ok":
                    solution = data.get("solution", {})
                    html = solution.get("response")
                    
                    # ç®€å•æ£€æŸ¥æ˜¯å¦çœŸçš„æ‹¿åˆ°äº†å†…å®¹ï¼Œè€Œä¸æ˜¯ blocked é¡µé¢
                    if "Just a moment" in html and len(html) < 5000:
                         logger.warning("   âš ï¸ Still stuck on Cloudflare challenge.")
                         self.destroy_session()
                         continue
                         
                    return html
                else:
                    logger.error(f"FlareSolverr request failed: {data}")
                    self.destroy_session() # å¤±è´¥å°±é”€æ¯ï¼Œä¿æŒç¯å¢ƒå¹²å‡€
                    
            except Exception as e:
                logger.error(f"Error fetching page {url}: {e}")
                self.destroy_session()
                
        return None
    
    def destroy_session(self):
        """Destroy the FlareSolverr session"""
        if self.session:
            try:
                requests.post(f"{self.base_url}/v1", json={
                    "cmd": "sessions.destroy",
                    "session": self.session
                }, timeout=10)
                logger.info(f"Destroyed FlareSolverr session: {self.session}")
            except Exception as e:
                logger.error(f"Error destroying session: {e}")
            finally:
                self.session = None

class EasyScholarCrawler:
    """Crawler for EasyScholar API - ç´«è‰²åˆ†åŒºã€çº¢è‰²åˆ†åŒºã€ç´«è‰²åˆ†æ•°"""
    
    def __init__(self, secret_key: str):
        self.api_url = "https://www.easyscholar.cc/open/getPublicationRank"
        self.secret_key = secret_key
        
    def get_journal_rank(self, journal_name: str) -> Dict[str, Any]:
        """
        è·å–æœŸåˆŠæ’åæ•°æ®
        
        Args:
            journal_name: æœŸåˆŠåç§°
            
        Returns:
            {
                'jcr_quartile': 'Q1',      # ç´«è‰²åˆ†åŒº
                'cas_division': 'æ•™è‚²å­¦2åŒº',# çº¢è‰²åˆ†åŒº
                'impact_factor': '5.4'     # ç´«è‰²åˆ†æ•°
            }
        """
        try:
            logger.info(f"   ğŸ” [EasyScholar] æŸ¥è¯¢æœŸåˆŠ: {journal_name}")
            
            response = requests.get(
                self.api_url,
                params={
                    'secretKey': self.secret_key,
                    'publicationName': journal_name
                },
                timeout=30
            )
            response.raise_for_status()
            data = response.json()
            
            if data.get('code') == 200:
                official_rank = data.get('data', {}).get('officialRank', {}).get('select', {})
                
                result = {
                    'purple_quartile': official_rank.get('ssci', ''),      # SSCI/SCIåˆ†åŒº
                    'red_division': official_rank.get('sciUp', ''),     # ä¸­ç§‘é™¢åˆ†åŒº
                    'purple_score': official_rank.get('sciif', '')     # Impact Factor
                }
                
                logger.info(f"   âœ… [EasyScholar] ç´«è‰²åˆ†åŒº={result['purple_quartile']}, "
                          f"çº¢è‰²åˆ†åŒº={result['red_division']}, ç´«è‰²åˆ†æ•°={result['purple_score']}")
                
                # å¿…é¡»çš„ 0.5 ç§’å»¶è¿Ÿ
                time.sleep(0.5)
                
                return result
            else:
                logger.warning(f"   âš ï¸ [EasyScholar] API é”™è¯¯: {data.get('msg')}")
                time.sleep(0.5)  # å³ä½¿å¤±è´¥ä¹Ÿå»¶è¿Ÿ
                return {}
                
        except Exception as e:
            logger.error(f"   âŒ [EasyScholar] è°ƒç”¨å¤±è´¥: {e}")
            time.sleep(0.5)  # ç¡®ä¿æ€»æ˜¯å»¶è¿Ÿ
            return {}


class PublisherCrawler:
    """Base class for publisher crawlers"""
    
    def __init__(self, flaresolverr_client: FlareSolverrClient):
        self.client = flaresolverr_client
    
    def extract_metrics(self, url: str) -> Dict[str, Any]:
        """Extract metrics from publisher page - to be implemented by subclasses"""
        return {}

class WileyCrawler(PublisherCrawler):
    """Crawler for Wiley journals - ä¼˜åŒ–ç‰ˆ"""
    
    def extract_metrics(self, url: str) -> Dict[str, Any]:
        """Extract metrics from Wiley journal-metrics page"""
        # ç¡®ä¿ URL æŒ‡å‘ metrics é¡µé¢
        if "journal-metrics" not in url:
            # å¤„ç†ç±»ä¼¼ /journal/1234/ çš„ URL
            if "/journal/" in url:
                url = url.replace("/journal/", "/journal-metrics/")
            # å¦‚æœç»“å°¾ä¸æ˜¯ metrics
            if not url.endswith("journal-metrics") and "journal-metrics" not in url:
                url = f"{url.rstrip('/')}/journal-metrics"
        
        logger.info(f"Fetching Wiley data from: {url}")
        html = self.client.get_page(url)
        if not html:
            return {}
        
        metrics = {
            'acceptance_rate': '',
            'first_decision_time': '',
            'review_time': '', # è¿™ä¸ªæŒ‡æ ‡åœ¨ä½ æä¾›çš„HTMLä¸­ä¸å­˜åœ¨ï¼Œä½œä¸ºé¢„ç•™
            'acceptance_time': '',
            'publication_time': '',
            'publisher': 'Wiley'
        }
        
        try:
            # ä½¿ç”¨ re.DOTALL è®© . å¯ä»¥åŒ¹é…æ¢è¡Œç¬¦
            # ä½¿ç”¨ re.IGNORECASE å¿½ç•¥å¤§å°å†™
            
            # 1. Extract Acceptance rate
            # HTML: <span class="label">Acceptance rate: </span></h4><p> 11%</p>
            # é€»è¾‘: æ‰¾åˆ° "Acceptance rate"ï¼Œè·³è¿‡ä¸­é—´æ‰€æœ‰å­—ç¬¦ç›´åˆ°é‡åˆ° <p>ï¼Œç„¶åæå–æ•°å­—
            ar_match = re.search(r'Acceptance\s+rate.*?<p>\s*(\d+(?:\.\d+)?)%', html, re.DOTALL | re.IGNORECASE)
            if ar_match:
                metrics['acceptance_rate'] = f"{ar_match.group(1)}%"
            
            # 2. Extract Submission to first decision
            # HTML: <span class="label">Submission to first decision <span> (median) </span>: </span></h4><p> 29 days </p>
            # é€»è¾‘: è¿™é‡Œçš„ .*? ä¼šè‡ªåŠ¨è·³è¿‡ä¸­é—´çš„ <span> (median) </span> ç»“æ„
            sfd_match = re.search(r'Submission\s+to\s+first\s+decision.*?<p>\s*(\d+)\s*days', html, re.DOTALL | re.IGNORECASE)
            if sfd_match:
                metrics['first_decision_time'] = f"{sfd_match.group(1)} days"
            
            # 3. Extract Submission to decision after review
            # æ³¨æ„ï¼šä½ æä¾›çš„ HTML ä¸­æ²¡æœ‰è¿™ä¸€é¡¹ï¼Œä½†å¦‚æœå…¶ä»– Wiley æœŸåˆŠæœ‰ï¼Œè¿™ä¸ªæ­£åˆ™å¯ä»¥åŒ¹é…
            sdar_match = re.search(r'Submission\s+to\s+decision\s+after\s+review.*?<p>\s*(\d+)\s*days', html, re.DOTALL | re.IGNORECASE)
            if sdar_match:
                metrics['review_time'] = f"{sdar_match.group(1)} days"
            
            # 4. Extract Submission to acceptance
            # HTML: <span class="label">Submission to acceptance <span> (median) </span>: </span></h4><p> 214 days </p>
            sa_match = re.search(r'Submission\s+to\s+acceptance.*?<p>\s*(\d+)\s*days', html, re.DOTALL | re.IGNORECASE)
            if sa_match:
                metrics['acceptance_time'] = f"{sa_match.group(1)} days"
            
            # 5. Extract Acceptance to publication
            # HTML: <span class="label">Acceptance to publication <span> (median) </span>: </span></h4><p> 15 days </p>
            ap_match = re.search(r'Acceptance\s+to\s+publication.*?<p>\s*(\d+)\s*days', html, re.DOTALL | re.IGNORECASE)
            if ap_match:
                metrics['publication_time'] = f"{ap_match.group(1)} days"
            
            logger.info(f"Extracted Wiley metrics: {metrics}")
            
        except Exception as e:
            logger.error(f"Error parsing Wiley HTML: {e}")
        
        return metrics
    
class TaylorFrancisCrawler(PublisherCrawler):
    """Crawler for Taylor & Francis journals - æœ€ç»ˆé˜²è´ªå©ªåŒ¹é…ç‰ˆ"""
    
    def extract_metrics(self, url: str) -> Dict[str, Any]:
        """Extract metrics from Taylor & Francis about-this-journal page"""
        if "about-this-journal" not in url:
            base_url = url.split('#')[0].rstrip('/')
            url = f"{base_url}/about-this-journal"
        
        # åŠ ä¸Šé”šç‚¹æ–¹ä¾¿æ—¥å¿—æ’æŸ¥
        if "#aims-and-scope" not in url:
            url = f"{url}#aims-and-scope"
        
        logger.info(f"Fetching Taylor & Francis data from: {url}")
        html = self.client.get_page(url)
        if not html:
            return {}
        
        metrics = {
            'acceptance_rate': '',
            'first_decision_time': '',
            'review_time': '',
            'publication_time': '',
            'acceptance_time': '',
            'publisher': 'Taylor & Francis'
        }
        
        try:
            # æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨ (?:(?!<strong>).)*? ä»£æ›¿ .*?
            # ä½œç”¨ï¼šåœ¨å¯»æ‰¾å…³é”®è¯æ—¶ï¼Œç¦æ­¢è·¨è¶Šä¸‹ä¸€ä¸ª <strong> æ ‡ç­¾ï¼Œé˜²æ­¢åŒ¹é…åˆ°ä¸Šé¢é”™è¯¯çš„æ•°å­—ã€‚
            
            # 1. Acceptance rate
            ar_match = re.search(r'<strong>\s*(\d+(?:\.\d+)?)\s*%?\s*</strong>(?:(?!<strong>).)*?acceptance\s+rate', html, re.DOTALL | re.IGNORECASE)
            if ar_match and float(ar_match.group(1)) > 0:
                metrics['acceptance_rate'] = f"{ar_match.group(1)}%"
            
            # 2. Submission to first decision (æˆªå›¾é‡Œæ˜¯0ï¼Œä¼šè¢«è¿‡æ»¤æ‰)
            sfd_match = re.search(r'<strong>\s*(\d+)\s*</strong>(?:(?!<strong>).)*?submission\s+to\s+first\s+decision', html, re.DOTALL | re.IGNORECASE)
            if sfd_match and sfd_match.group(1) != '0':
                metrics['first_decision_time'] = f"{sfd_match.group(1)} days"
            
            # 3. Submission to post-review decision (æˆªå›¾é‡Œæ˜¯50)
            # ä¹‹å‰çš„ä»£ç ä¼šå› ä¸ºè´ªå©ªåŒ¹é…é”™è¯¯åœ°æŠ“æˆ 0ï¼Œç°åœ¨ä¼šæ­£ç¡®æŠ“åˆ° 50
            sprd_match = re.search(r'<strong>\s*(\d+)\s*</strong>(?:(?!<strong>).)*?submission\s+to\s+first\s+post-review\s+decision', html, re.DOTALL | re.IGNORECASE)
            if sprd_match and sprd_match.group(1) != '0':
                metrics['review_time'] = f"{sprd_match.group(1)} days"
            
            # 4. Acceptance to online publication (æˆªå›¾é‡Œæ˜¯30)
            ap_match = re.search(r'<strong>\s*(\d+)\s*</strong>(?:(?!<strong>).)*?acceptance\s+to\s+online\s+publication', html, re.DOTALL | re.IGNORECASE)
            if ap_match and ap_match.group(1) != '0':
                metrics['publication_time'] = f"{ap_match.group(1)} days"

            # 5. Submission to acceptance (æˆªå›¾é‡Œæ²¡æœ‰)
            sa_match = re.search(r'<strong>\s*(\d+)\s*</strong>(?:(?!<strong>).)*?submission\s+to\s+acceptance', html, re.DOTALL | re.IGNORECASE)
            if sa_match and sa_match.group(1) != '0':
                 metrics['acceptance_time'] = f"{sa_match.group(1)} days"

            logger.info(f"Extracted Taylor & Francis metrics: {metrics}")
            
        except Exception as e:
            logger.error(f"Error parsing Taylor & Francis HTML: {e}")
        
        return metrics

class SpringerCrawler(PublisherCrawler):
    """Crawler for Springer journals - ä¼˜åŒ–ç‰ˆ"""
    
    def extract_metrics(self, url: str) -> Dict[str, Any]:
        """Extract metrics from Springer journal page"""
        logger.info(f"Fetching Springer data from: {url}")
        html = self.client.get_page(url)
        if not html:
            return {}
        
        metrics = {
            'first_decision_time': '',
            'publisher': 'Springer'
        }
        
        try:
            # ä½¿ç”¨ re.DOTALL è·¨è¡ŒåŒ¹é…
            # ç­–ç•¥ï¼šåˆ©ç”¨ Springer ç‰¹æœ‰çš„ data-test å±æ€§å®šä½ï¼Œå¿½ç•¥ä¸­é—´çš„ HTML æ ‡ç­¾ç»“æ„
            
            # 1. Extract Submission to first decision
            # HTML: <dd data-test="metrics-speed-value"> \n <span class="u-text-bold">19 days</span> \n </dd>
            # é€»è¾‘ï¼šæ‰¾åˆ° metrics-speed-valueï¼Œä¸ç®¡ä¸­é—´éš”äº†å¤šå°‘æ ‡ç­¾(<span>ç­‰)ï¼Œç›´æ¥æ‰¾åé¢çš„æ•°å­— + days
            speed_match = re.search(r'data-test="metrics-speed-value".*?(\d+)\s*days', html, re.DOTALL | re.IGNORECASE)
            if speed_match:
                metrics['first_decision_time'] = f"{speed_match.group(1)} days"
            
            logger.info(f"Extracted Springer metrics: {metrics}")
            
        except Exception as e:
            logger.error(f"Error parsing Springer HTML: {e}")
        
        return metrics

class SageCrawler(PublisherCrawler):
    """Crawler for SAGE journals - ä¼˜åŒ–ç‰ˆ"""
    
    def extract_metrics(self, url: str) -> Dict[str, Any]:
        """Extract metrics from SAGE journal page"""
        logger.info(f"Fetching SAGE data from: {url}")
        html = self.client.get_page(url)
        if not html:
            return {}
        
        metrics = {
            'first_decision_time': '',
            'publication_time': '',
            'acceptance_rate': '',
            'publisher': 'SAGE'
        }
        
        try:
            # ä½¿ç”¨ re.DOTALL å¿½ç•¥æ¢è¡Œç¬¦å½±å“
            # ä½¿ç”¨ re.IGNORECASE å¿½ç•¥å¤§å°å†™
            
            # 1. Extract First decision -> æ˜ å°„åˆ° first_decision_time
            # HTML: First decision:</div><div ...>77<span>days*</span>
            # é€»è¾‘: æ‰¾åˆ° "First decision:"ï¼Œè·³è¿‡ä¸­é—´ä¹±ä¸ƒå…«ç³Ÿçš„æ ‡ç­¾ï¼Œæ‰¾åˆ°æ•°å­—ï¼Œä¸”ç¡®ä¿åé¢è·Ÿç€ days
            fd_match = re.search(r'First\s+decision:.*?(\d+)\s*<span[^>]*>days', html, re.DOTALL | re.IGNORECASE)
            if fd_match:
                metrics['first_decision_time'] = f"{fd_match.group(1)} days"
            
            # 2. Extract Acceptance to publication
            # HTML: Acceptance to publication:</div><div ...>39<span>days*</span>
            ap_match = re.search(r'Acceptance\s+to\s+publication:.*?(\d+)\s*<span[^>]*>days', html, re.DOTALL | re.IGNORECASE)
            if ap_match:
                metrics['publication_time'] = f"{ap_match.group(1)} days"
            
            # 3. Extract Acceptance rate
            # HTML: Acceptance rate:</div><div ...>5.0<span class="percentage">%</span>
            # é€»è¾‘: åŒ¹é…æ•´æ•°æˆ–å°æ•° (å¦‚ 5 æˆ– 5.0)ï¼Œä¸”åé¢è·Ÿç€ %
            ar_match = re.search(r'Acceptance\s+rate:.*?(\d+(?:\.\d+)?)\s*<span[^>]*>%', html, re.DOTALL | re.IGNORECASE)
            if ar_match:
                metrics['acceptance_rate'] = f"{ar_match.group(1)}%"
            
            logger.info(f"Extracted SAGE metrics: {metrics}")
            
        except Exception as e:
            logger.error(f"Error parsing SAGE HTML: {e}")
        
        return metrics

import re
from typing import Dict, Any

class ElsevierCrawler(PublisherCrawler):
    """Crawler for Elsevier journals - ä¼˜åŒ–ç‰ˆ"""
    
    def extract_metrics(self, url: str) -> Dict[str, Any]:
        """Extract metrics from Elsevier insights page"""
        logger.info(f"Fetching Elsevier data from: {url}")
        html = self.client.get_page(url)
        if not html:
            return {}
        
        # å­—æ®µåˆå§‹åŒ– - ä½¿ç”¨ä¸‹åˆ’çº¿å‘½åä¸å…¶ä»–çˆ¬è™«ä¿æŒä¸€è‡´
        metrics = {
            'acceptance_rate': '',
            'first_decision_time': '',
            'review_time': '',
            'acceptance_time': '',
            'publication_time': '', 
            'publisher': 'Elsevier'
        }
        
        def clean_value(raw_value: str) -> str:
            """æ¸…æ´— Elsevier ç‰¹æœ‰çš„è„æ•°æ®ï¼Œå¦‚ '8<!-- --> days' -> '8 days'"""
            if not raw_value:
                return ''
            # 1. ç§»é™¤ HTML æ³¨é‡Š <!-- -->
            cleaned = re.sub(r'<!--.*?-->', '', raw_value)
            # 2. åˆå¹¶å¤šä½™ç©ºæ ¼
            cleaned = re.sub(r'\s+', ' ', cleaned)
            # 3. å»é™¤é¦–å°¾ç©ºæ ¼
            return cleaned.strip()

        # æ ‡ç­¾ -> å­—æ®µåçš„æ˜ å°„è¡¨ï¼ˆä½¿ç”¨ä¸‹åˆ’çº¿å‘½åï¼‰
        LABEL_MAPPING = {
            'submission to first decision': 'first_decision_time',
            'first decision': 'first_decision_time',
            'submission to first decision (median)': 'first_decision_time',
            'submission to decision after review': 'review_time',
            'submission to acceptance': 'acceptance_time',
            'acceptance to online publication': 'publication_time',
            'acceptance to publication': 'publication_time',
            'acceptance rate': 'acceptance_rate',
        }

        try:
            # ===== æ ¸å¿ƒæ­£åˆ™ï¼šåŒ¹é… metric-box å†…çš„ å€¼+æ ‡ç­¾ =====
            # ç»“æ„ï¼š<li class="metric-box..."><span class="text-xl">å€¼</span>...<div class="text-s">æ ‡ç­¾</div>
            pattern = r'<li[^>]*class="metric-box[^"]*"[^>]*>.*?' \
                      r'<span[^>]*class="text-xl"[^>]*>(.*?)</span>.*?' \
                      r'<div[^>]*class="text-s"[^>]*>(.*?)</div>'
            
            matches = re.findall(pattern, html, re.DOTALL | re.IGNORECASE)
            
            for raw_value, raw_label in matches:
                cleaned_value = clean_value(raw_value)
                cleaned_label = clean_value(raw_label).lower()
                
                # åœ¨æ˜ å°„è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”å­—æ®µ
                for label_key, field_name in LABEL_MAPPING.items():
                    if label_key in cleaned_label:
                        metrics[field_name] = cleaned_value
                        logger.debug(f"Matched: '{cleaned_label}' -> {field_name} = {cleaned_value}")
                        break
            
            logger.info(f"Extracted Elsevier metrics: {metrics}")
            
        except Exception as e:
            import traceback
            logger.error(f"Error parsing Elsevier HTML: {e}")
            logger.error(traceback.format_exc())
        
        return metrics

class JournalRankingUpdater:
    def __init__(self, flaresolverr_url: str = FLARESOLVERR_URL, easyscholar_key: str = None):
        self.flaresolverr_client = FlareSolverrClient(flaresolverr_url)
        
        # Initialize EasyScholar crawler if key is provided
        if easyscholar_key:
            self.easyscholar_crawler = EasyScholarCrawler(easyscholar_key)
            logger.info("EasyScholar API initialized")
        else:
            self.easyscholar_crawler = None
            logger.warning("âš ï¸ No EasyScholar API key provided - ç´«è‰²åˆ†åŒºã€çº¢è‰²åˆ†åŒºã€ç´«è‰²åˆ†æ•° will not be updated from EasyScholar")
        
        # Initialize publisher crawlers
        self.publisher_crawlers = {
            'wiley': WileyCrawler(self.flaresolverr_client),
            'taylor_francis': TaylorFrancisCrawler(self.flaresolverr_client),
            'springer': SpringerCrawler(self.flaresolverr_client),
            'sage': SageCrawler(self.flaresolverr_client),
            'elsevier': ElsevierCrawler(self.flaresolverr_client)
        }
        
        # Map publishers to crawlers
        self.publisher_map = {
            'wiley.com': 'wiley',
            'onlinelibrary.wiley.com': 'wiley',
            'tandfonline.com': 'taylor_francis',
            'springer.com': 'springer',
            'link.springer.com': 'springer',
            'sagepub.com': 'sage',
            'journals.sagepub.com': 'sage',
            'sciencedirect.com': 'elsevier',
            'elsevier.com': 'elsevier'
        }
    
    def load_journal_data(self):
        """Load journal data from journal_rank.json and jrank.yml"""
        journal_rank_file = '_data/journal_rank.json'
        jrank_file = '_data/jrank.yml'
        
        # Check if journal_rank.json exists
        if not os.path.exists(journal_rank_file):
            logger.error(f"Required file not found: {journal_rank_file}")
            logger.error("Please ensure journal_rank.json exists in the _data directory")
            return [], []
        
        try:
            with open(journal_rank_file, 'r', encoding='utf-8') as f:
                journal_list = json.load(f)
                logger.info(f"Loaded {len(journal_list)} journals from {journal_rank_file}")
            
            # Try to load existing data from jrank.yml
            try:
                with open(jrank_file, 'r', encoding='utf-8') as f:
                    existing_data = yaml.safe_load(f) or []
                    logger.info(f"Loaded {len(existing_data)} existing entries from {jrank_file}")
            except FileNotFoundError:
                logger.info(f"{jrank_file} not found, will create new file")
                existing_data = []
                
            return journal_list, existing_data
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in {journal_rank_file}: {e}")
            return [], []
        except Exception as e:
            logger.error(f"Error loading journal data: {e}")
            return [], []
    
    def get_publisher_from_url(self, url: str) -> Optional[str]:
        """Determine publisher from URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # Remove www. prefix if present
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # Check for known publisher domains
            for publisher_domain, publisher_key in self.publisher_map.items():
                if publisher_domain in domain:
                    return publisher_key
            
            # Special cases
            if 'springer' in domain:
                return 'springer'
            elif 'wiley' in domain:
                return 'wiley'
            elif 'tandf' in domain:
                return 'taylor_francis'
            elif 'sage' in domain:
                return 'sage'
            elif 'elsevier' in domain or 'sciencedirect' in domain:
                return 'elsevier'
            
            return None
        except Exception as e:
            logger.error(f"Error determining publisher from URL {url}: {e}")
            return None
    
    def update_journal_rankings(self, dry_run: bool = False):
        """Main function to update all journal rankings"""
        if dry_run:
            logger.info("Running in DRY-RUN mode - data will NOT be saved")
        
        journal_list, existing_data = self.load_journal_data()
        
        # Create a dictionary for quick lookup of existing data
        existing_dict = {item['journal']: item for item in existing_data}
        
        updated_count = 0
        
        for journal_info in journal_list:
            journal_name = journal_info['name']
            url = journal_info.get('url', '')
            sourceid = journal_info.get('sourceid')
            tags = journal_info.get('tag', [])
            
            logger.info(f"Processing {journal_name}...")
            
            # è·å–ç°æœ‰æ•°æ®æˆ–åˆ›å»ºæ–°æ¡ç›®ï¼ˆä¿ç•™æ‰€æœ‰ç°æœ‰å­—æ®µï¼‰
            if journal_name in existing_dict:
                journal_data = existing_dict[journal_name].copy()
                # æ›´æ–° tagï¼ˆå¦‚æœæœ‰æ–°çš„ï¼‰
                if tags and not journal_data.get('tag'):
                    journal_data['tag'] = tags
            else:
                # æ–°æœŸåˆŠï¼Œåˆ›å»ºåŸºç¡€æ¡ç›®
                journal_data = {
                    'journal': journal_name,
                    'publisher': '',
                    'tag': tags,
                    'purple_quartile': '',
                    'orange_quartile': '',
                    'orange_percentile': '',
                    'red_division': '',
                    'orange_score': '',
                    'documents_published': '',
                    'purple_score': '',
                    'acceptance_rate': '',
                    'first_decision_time': '',
                    'review_time': '',
                    'acceptance_time': '',
                    'publication_time': '',
                    'hm_score': ''
                }
            
            # Determine publisher from URL
            if url:
                publisher_key = self.get_publisher_from_url(url)
                if publisher_key:
                    journal_data['publisher'] = publisher_key
            
            # Get publisher-specific metrics
            if url and journal_data.get('publisher'):
                publisher_key = journal_data['publisher']
                if publisher_key in self.publisher_crawlers:
                    try:
                        publisher_metrics = self.publisher_crawlers[publisher_key].extract_metrics(url)
                        # Update only if we got data
                        for key, value in publisher_metrics.items():
                            if value:
                                journal_data[key] = value
                    except Exception as e:
                        logger.error(f"Error getting publisher metrics for {journal_name}: {e}")
            
            # Get EasyScholar data (ç´«è‰²åˆ†åŒºã€çº¢è‰²åˆ†åŒºã€ç´«è‰²åˆ†æ•°) - ä¼˜å…ˆçº§æœ€é«˜
            if self.easyscholar_crawler:
                try:
                    easyscholar_data = self.easyscholar_crawler.get_journal_rank(journal_name)
                    
                    # æ›´æ–° 3 ä¸ªå­—æ®µï¼ˆEasyScholar æ•°æ®ä¼˜å…ˆçº§æœ€é«˜ï¼Œä¼šè¦†ç›–ä¹‹å‰çš„å€¼ï¼‰
                    if easyscholar_data.get('purple_quartile'):
                        journal_data['purple_quartile'] = easyscholar_data['purple_quartile']
                    if easyscholar_data.get('red_division'):
                        journal_data['red_division'] = easyscholar_data['red_division']
                    if easyscholar_data.get('purple_score'):
                        journal_data['purple_score'] = easyscholar_data['purple_score']
                        
                except Exception as e:
                    logger.error(f"Error getting EasyScholar data for {journal_name}: {e}")
            
            # Calculate HM score
            journal_data['hm_score'] = self.calculate_hm_score(journal_data)
            
            # æ›´æ–°åˆ° existing_dict
            existing_dict[journal_name] = journal_data
            updated_count += 1
            
            # Add delay to avoid rate limiting
            time.sleep(random.uniform(2, 5))
        
        # Save updated data (skip if dry-run or no updates)
        if dry_run:
            logger.info("DRY-RUN: Skipping file save. Would have updated %d journals", updated_count)
            logger.info("DRY-RUN: Sample data (first journal):")
            if existing_dict:
                first_journal = list(existing_dict.values())[0]
                logger.info(yaml.dump([first_journal], default_flow_style=False, allow_unicode=True))
        elif updated_count == 0:
            logger.info("â„¹ï¸ æ²¡æœ‰æ•°æ®æ›´æ–°ï¼Œè·³è¿‡ä¿å­˜")
        else:
            try:
                # è½¬æ¢å›åˆ—è¡¨ï¼ˆä¿ç•™æ‰€æœ‰æœŸåˆŠæ•°æ®ï¼‰
                updated_data = list(existing_dict.values())
                with open('_data/jrank.yml', 'w', encoding='utf-8') as f:
                    yaml.dump(updated_data, f, default_flow_style=False, allow_unicode=True)
                logger.info("Successfully updated jrank.yml with %d journals", len(updated_data))
            except Exception as e:
                logger.error(f"Error saving updated data: {e}")
        
        # Clean up FlareSolverr session
        self.flaresolverr_client.destroy_session()
    
    def calculate_hm_score(self, journal_data):
        """Calculate HM (Haoming) custom score based on multiple factors
        
        è®¡ç®—å…¬å¼:
        - ç´«è‰²åˆ†åŒº (purple_quartile): 20åˆ† (Q1=20, Q2=15, Q3=10, Q4=5)
        - ç´«è‰²åˆ†æ•° (purple_score): ç›´æ¥åŠ 
        - æ©™è‰²åˆ†æ•° (orange_score): é™¤ä»¥2ç›´æ¥åŠ 
        - æ©™è‰²ç™¾åˆ†ä½ (orange_percentile): 20åˆ† (æŒ‰ç™¾åˆ†æ¯”è®¡ç®—)
        - æ¥å—ç‡ (acceptance_rate): åå‘åŠ åˆ† (100-rate)/10ï¼Œé»˜è®¤4åˆ†
        - å‘æ–‡é‡ (documents_last_year): >200åŠ 10åˆ†, >100åŠ 5åˆ†, >50åŠ 3åˆ†
        """
        score = 0
        
        # 1. ç´«è‰²åˆ†åŒº scoring (20åˆ†æ»¡åˆ†)
        jcr = journal_data.get('purple_quartile', '').upper()
        if 'Q1' in jcr:
            score += 20
        elif 'Q2' in jcr:
            score += 15
        elif 'Q3' in jcr:
            score += 10
        elif 'Q4' in jcr:
            score += 5
            
        # 2. ç´«è‰²åˆ†æ•° - ä¹˜ä»¥2ç›´æ¥åŠ 
        try:
            purple_score = float(journal_data.get('purple_score', 0) or 0)
            score += purple_score * 2
        except (ValueError, TypeError):
            pass
            
        # 3. æ©™è‰²åˆ†æ•° - ç›´æ¥åŠ 
        try:
            orange_score = float(journal_data.get('orange_score', 0) or 0)
            score += orange_score
        except (ValueError, TypeError):
            pass
            
        # 4. æ©™è‰²ç™¾åˆ†ä½ scoring (20åˆ†æ»¡åˆ†ï¼ŒæŒ‰ç™¾åˆ†æ¯”è®¡ç®—)
        try:
            orange_percentile = float(journal_data.get('orange_percentile', 0) or 0)
            score += orange_percentile * 0.2  # 99 -> 19.8, 50 -> 10
        except (ValueError, TypeError):
            pass
            
        # 5. æ¥å—ç‡ - ç›´æ¥åŠ åˆ†ï¼Œé»˜è®¤4åˆ†
        try:
            acceptance_rate = journal_data.get('acceptance_rate', '')
            if acceptance_rate and '%' in str(acceptance_rate):
                rate = float(str(acceptance_rate).replace('%', ''))
                # æ¥å—ç‡ç›´æ¥åŠ : 10% -> 10åˆ†, 20% -> 20åˆ†
                score += rate
            else:
                # æ²¡æœ‰æ¥å—ç‡æ•°æ®ï¼Œé»˜è®¤åŠ 4åˆ†
                score += 4
        except (ValueError, TypeError):
            score += 4
            
        # 6. å‘æ–‡é‡åŠ åˆ† (documents_last_year)
        try:
            docs_last_year = journal_data.get('documents_last_year', '')
            if docs_last_year:
                # æ ¼å¼å¯èƒ½æ˜¯ "127 (2024)" æˆ–çº¯æ•°å­—
                docs_str = str(docs_last_year).split('(')[0].strip()
                docs_count = int(docs_str)
                if docs_count > 200:
                    score += 10
                elif docs_count > 100:
                    score += 5
                elif docs_count > 50:
                    score += 3
        except (ValueError, TypeError):
            pass
            
        return round(score, 1)  # å››èˆäº”å…¥åˆ°å°æ•°ç‚¹å1ä½

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Update journal ranking data using FlareSolverr and EasyScholar API')
    parser.add_argument('--flaresolverr', '-f', type=str, default=FLARESOLVERR_URL,
                       help=f'FlareSolverr URL (default: {FLARESOLVERR_URL})')
    parser.add_argument('--easyscholar-key', '-e', type=str, 
                       help='EasyScholar API secret key (can also use EASYSCHOLAR_KEY env variable)')
    parser.add_argument('--debug', '-d', action='store_true', help='Enable debug logging')
    parser.add_argument('--dry-run', '-n', action='store_true', 
                       help='Dry run - collect data but don\'t save')
    args = parser.parse_args()
    
    # Set logging level
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Get EasyScholar key from args or environment variable
    easyscholar_key = args.easyscholar_key or os.environ.get('EASYSCHOLAR_KEY')
    
    # Create and run updater
    updater = JournalRankingUpdater(args.flaresolverr, easyscholar_key=easyscholar_key)
    
    try:
        logger.info("Starting journal ranking update...")
        updater.update_journal_rankings(dry_run=args.dry_run)
        logger.info("Journal ranking update completed successfully")
    except KeyboardInterrupt:
        logger.info("Update interrupted by user")
        # Clean up session
        updater.flaresolverr_client.destroy_session()
        sys.exit(1)
    except Exception as e:
        logger.error(f"Update failed with error: {e}")
        # Clean up session
        updater.flaresolverr_client.destroy_session()
        sys.exit(1)

if __name__ == "__main__":
    main()
