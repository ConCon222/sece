#!/usr/bin/env python3
"""
æœŸåˆŠæŒ‡æ ‡æ›´æ–°å™¨ - ä½¿ç”¨ DrissionPage è·å–æ©™è‰²åˆ†æ•°æŒ‡æ ‡
ç‹¬ç«‹è¿è¡Œï¼Œä¸“é—¨æ›´æ–°ï¼šæ©™è‰²åˆ†æ•°ã€æ©™è‰²åˆ†åŒºã€Documents Publishedã€Percentile
"""

import json
import yaml
import time
import logging
from DrissionPage import WebPage, ChromiumOptions
from typing import Dict, Any, Optional
import re

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ScopusDrissionCrawler:
    """ä½¿ç”¨ DrissionPage çˆ¬å–æœŸåˆŠæ©™è‰²ç³»æŒ‡æ ‡"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.base_url = "https://www.scopus.com/sourceid"
        
        # é…ç½®æµè§ˆå™¨é€‰é¡¹
        self.options = ChromiumOptions()
        
        # 1. å…³é”®ï¼šè®¾ç½®æ— å¤´æ¨¡å¼çš„ç‰¹å®šå‚æ•°ä»¥é˜²è¢«æ£€æµ‹
        if headless:
            self.options.headless()
            # æŸäº›åçˆ¬è™«æ£€æµ‹æ— å¤´æµè§ˆå™¨çš„ user-agentï¼Œæ‰‹åŠ¨å¼ºåˆ¶è¦†ç›–
            self.options.set_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # 2. é…ç½®ä»£ç† (å¦‚æœæœ¬åœ° IP è¢«å°)
        # æ ¼å¼: http://username:password@ip:port æˆ– http://ip:port
        # self.options.set_proxy('http://127.0.0.1:7890') 
        
        # 3. è‡ªåŠ¨å¤„ç†ä¸€äº›åçˆ¬ç‰¹å¾
        self.options.auto_port()  # è‡ªåŠ¨å¯»æ‰¾å¯ç”¨ç«¯å£
        self.options.set_argument('--no-sandbox')
        self.options.set_argument('--disable-gpu')
        
        # DrissionPage é»˜è®¤å·²ç»å¤„ç†äº†å¾ˆå¤š WebDriver ç‰¹å¾ï¼Œé€šå¸¸ä¸éœ€è¦åƒ Selenium é‚£æ ·åšå¾ˆå¤š mask
    
    def calculate_orange_quartile(self, percentile: float) -> str:
        """
        æ ¹æ® Percentile è®¡ç®—æ©™è‰²åˆ†åŒº
        Q1: 100-75
        Q2: 74-50
        Q3: 49-25
        Q4: 24-0
        """
        if percentile >= 75:
            return "Q1"
        elif percentile >= 50:
            return "Q2"
        elif percentile >= 25:
            return "Q3"
        else:
            return "Q4"
    
    def scrape_journal_metrics(self, source_id: int) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªæœŸåˆŠçš„ Scopus æŒ‡æ ‡
        
        Args:
            source_id: Scopus æœŸåˆŠ ID
            
        Returns:
            {
                'citescore': '17.6',
                'sjr_quartile': 'Q1',
                'sjr_percentile': '95',
                'documents_published': '133'
            }
        """
        result = {
        "source_id": source_id,
        "orange_score": None,
        "orange_quartile": None,
        "orange_percentile": None,
        "documents_published": None,
        "docs_current_year": None,  # æ–°å¢ï¼šå½“å¹´å‘æ–‡é‡
        "docs_last_year": None,     # æ–°å¢ï¼šå»å¹´å‘æ–‡é‡
        "citescore_rank_data": [],
        "documents_data": [],
        "success": False,
        "error": None
    }
        
        # åˆ›å»º WebPage å®ä¾‹ï¼Œåº”ç”¨é…ç½®
        page = WebPage(chromium_options=self.options)
        
        try:
            # 1. è®¿é—® Scopus æœŸåˆŠé¡µé¢ (tabs=0 æ˜¾ç¤º CiteScore)
            url = f"{self.base_url}/{source_id}#tabs=0"
            logger.info(f"æ­£åœ¨è®¿é—®: {url}")
            page.get(url, timeout=30)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            page.wait.ele_displayed('#rpResult', timeout=20)
            time.sleep(2)
            
            # 2. æŠ“å– CiteScore
            try:
                citescore_element = page.ele('#rpResult')
                if citescore_element:
                    result['orange_score'] = citescore_element.text.strip()
                    logger.info(f"   âœ… æ©™è‰²åˆ†æ•°: {result['orange_score']}")
            except Exception as e:
                logger.warning(f"   âš ï¸ æ— æ³•è·å–æ©™è‰²åˆ†æ•°: {e}")
            
            # 3. æŠ“å– Percentile (ç”¨äºè®¡ç®— SJR Quartile)
            try:
                page.wait.ele_displayed('#rpCategoryDropDown', timeout=15)
                # æŸ¥æ‰¾åˆ†ç±»è¡¨æ ¼
                table = page.ele('#CSCategoryTBody')
                if table:
                    rows = table.eles('tag:tr')
                    if rows:
                        # å–ç¬¬ä¸€ä¸ªåˆ†ç±»çš„ Percentile
                        first_row = rows[0]
                        cells = first_row.eles('tag:td')
                        if len(cells) >= 3:
                            percentile_text = cells[2].text.strip()
                            # æå–æ•°å­—ï¼šå¦‚ "95th" -> "95"
                            percentile_match = re.search(r'(\d+)', percentile_text)
                            if percentile_match:
                                percentile = int(percentile_match.group(1))
                                result['orange_percentile'] = str(percentile)
                                result['orange_quartile'] = self.calculate_orange_quartile(percentile)
                                logger.info(f"   âœ… Percentile: {percentile}th -> æ©™è‰²åˆ†åŒº: {result['orange_quartile']}")
            except Exception as e:
                logger.warning(f"   âš ï¸ æ— æ³•è·å– Percentile: {e}")
            
            # ... (å‰é¢ä»£ç ä¿æŒä¸å˜) ...

            # 4. å¯¼èˆªåˆ° Content Coverage æ ‡ç­¾é¡µ (#tabs=2) è·å– Documents Published æ•°æ®
            print("\n=== æ­¥éª¤ 3: è·å– Documents Published æ•°æ® (å½“å¹´ & å»å¹´) ===")
            try:
                # ç¡®ä¿ result å­—å…¸é‡Œæœ‰è¿™ä¸¤ä¸ªå­—æ®µ (å»ºè®®åœ¨å‡½æ•°å¼€å¤´åˆå§‹åŒ–æ—¶åŠ ä¸Š)
                result["docs_current_year"] = "0"
                result["docs_last_year"] = "0"

                print("æ­£åœ¨å¯¼èˆªåˆ° Content Coverage æ ‡ç­¾é¡µ (#tabs=2)...")
                content_coverage_url = f"https://www.scopus.com/sourceid/{source_id}#tabs=2"
                page.get(content_coverage_url, timeout=30)
                time.sleep(3) # ç­‰å¾…æ¸²æŸ“
                
                page.wait.ele_displayed("#contentCoverage", timeout=20)
                
                # è·å–è¡¨æ ¼
                table = page.ele('#contentCoverage')
                rows = []
                if table:
                    rows = table.eles('tag:tr')
                
                print(f"æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®")
                
                # éå†å‰ä¸¤è¡Œï¼šç¬¬0è¡Œé€šå¸¸æ˜¯å½“å¹´(Header)ï¼Œç¬¬1è¡Œé€šå¸¸æ˜¯å»å¹´
                for i, row in enumerate(rows):
                    if i > 1: break # æˆ‘ä»¬åªéœ€è¦å‰ä¸¤å¹´ï¼Œæ‹¿åˆ°åå°±é€€å‡ºå¾ªç¯

                    cells = row.eles("tag:td")
                    if not cells: 
                         cells = row.eles("tag:th")

                    if len(cells) >= 2:
                        # æå–å¹´ä»½å’Œæ•°é‡
                        year_text = cells[0].text.strip()
                        doc_text = cells[1].text.strip()
                        
                        # ä½¿ç”¨æ­£åˆ™æå–çº¯æ•°å­— (å¤„ç† "176 documents" -> "176")
                        doc_count_match = re.search(r'(\d+)', doc_text)
                        doc_count = doc_count_match.group(1) if doc_count_match else "0"

                        print(f"è§£æç¬¬ {i} è¡Œ -> å¹´ä»½: {year_text} | æ•°é‡: {doc_count}")

                        # é€»è¾‘åˆ¤æ–­ï¼šç¬¬0è¡Œè§†ä¸ºâ€œæœ€æ–°/å½“å¹´â€ï¼Œç¬¬1è¡Œè§†ä¸ºâ€œå»å¹´â€
                        if i == 0:
                            result["docs_current_year"] = f"{doc_count} ({year_text})"
                            result["documents_data"].append({"year": year_text, "documents": doc_count})
                        elif i == 1:
                            result["docs_last_year"] = f"{doc_count} ({year_text})"
                            result["documents_data"].append({"year": year_text, "documents": doc_count})

                if result["documents_data"]:
                    print(f"âœ… æå–æˆåŠŸ")
                else:
                    print(f"âš ï¸ æœªæå–åˆ°æ•°æ®")

            except Exception as e:
                print(f"âœ— è·å– Documents Published æ•°æ®å¤±è´¥: {e}")
            
            # ... (åé¢ä»£ç ä¿æŒä¸å˜) ...
                
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        
        finally:
            # å…³é—­é¡µé¢
            try:
                page.quit()
            except:
                pass
        
        return result


def update_scopus_metrics_in_yaml(dry_run: bool = False):
    """
    æ›´æ–° jrank.yml ä¸­çš„æ©™è‰²ç³»æŒ‡æ ‡
    
    Args:
        dry_run: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼ˆä¸ä¿å­˜æ–‡ä»¶ï¼‰
    """
    journal_rank_file = '_data/journal_rank.json'
    jrank_file = '_data/jrank.yml'
    
    # 1. è¯»å–æœŸåˆŠåˆ—è¡¨ï¼ˆè·å– sourceidï¼‰
    try:
        with open(journal_rank_file, 'r', encoding='utf-8') as f:
            journal_list = json.load(f)
        logger.info(f"ğŸ“– åŠ è½½äº† {len(journal_list)} ä¸ªæœŸåˆŠ")
    except Exception as e:
        logger.error(f"âŒ æ— æ³•è¯»å– {journal_rank_file}: {e}")
        return
    
    # 2. è¯»å–ç°æœ‰çš„ jrank.yml
    try:
        with open(jrank_file, 'r', encoding='utf-8') as f:
            jrank_data = yaml.safe_load(f) or []
        logger.info(f"ğŸ“– åŠ è½½äº† {len(jrank_data)} æ¡ç°æœ‰æ•°æ®")
    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {jrank_file}")
        return
    except Exception as e:
        logger.error(f"âŒ æ— æ³•è¯»å– {jrank_file}: {e}")
        return
    
    # 3. åˆ›å»ºæœŸåˆŠåç§°åˆ°æ•°æ®çš„æ˜ å°„
    jrank_dict = {item['journal']: item for item in jrank_data}
    
    # 4. åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ScopusDrissionCrawler(headless=True)
    
    # 5. éå†æœŸåˆŠåˆ—è¡¨ï¼Œæ›´æ–°æ©™è‰²ç³»æŒ‡æ ‡
    updated_count = 0
    for journal_info in journal_list:
        journal_name = journal_info['name']
        sourceid = journal_info.get('sourceid')
        
        if not sourceid:
            logger.info(f"â© è·³è¿‡ {journal_name} (æ—  sourceid)")
            continue
        
        if journal_name not in jrank_dict:
            # è‡ªåŠ¨åˆ›å»ºæœŸåˆŠæ¡ç›®
            logger.info(f"â• åˆ›å»ºæ–°æ¡ç›®: {journal_name}")
            jrank_dict[journal_name] = {
                'journal': journal_name,
                'orange_score': '',
                'orange_quartile': '',
                'orange_percentile': '',
                'documents_current_year': '',
                'documents_last_year': '',
                'documents_published': ''
            }
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“Š å¤„ç†: {journal_name} (ID: {sourceid})")
        logger.info(f"{'='*80}")
        
        try:
            # çˆ¬å–æ©™è‰²ç³»æŒ‡æ ‡
            scopus_metrics = crawler.scrape_journal_metrics(sourceid)
            
            # æ›´æ–° jrank_dict ä¸­çš„æ•°æ®
            if scopus_metrics['orange_score']:
                jrank_dict[journal_name]['orange_score'] = scopus_metrics['orange_score']
            if scopus_metrics['orange_quartile']:
                jrank_dict[journal_name]['orange_quartile'] = scopus_metrics['orange_quartile']
            if scopus_metrics['orange_percentile']:
                jrank_dict[journal_name]['orange_percentile'] = scopus_metrics['orange_percentile']
            
            # ä½¿ç”¨ split åçš„å­—æ®µ
            if scopus_metrics['docs_current_year']:
                jrank_dict[journal_name]['documents_current_year'] = scopus_metrics['docs_current_year']
            if scopus_metrics['docs_last_year']:
                jrank_dict[journal_name]['documents_last_year'] = scopus_metrics['docs_last_year']
                # ä¿ç•™ documents_published ç”¨äºå…¼å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œæˆ–è€…å¯ä»¥åˆ é™¤
                jrank_dict[journal_name]['documents_published'] = scopus_metrics['docs_last_year']
            
            updated_count += 1
            logger.info(f"âœ… {journal_name} æ›´æ–°å®Œæˆ")
            
            # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
            
        except Exception as e:
            logger.error(f"âŒ {journal_name} æ›´æ–°å¤±è´¥: {e}")
    
    # 6. ä¿å­˜æ›´æ–°åçš„æ•°æ®
    if dry_run:
        logger.info("\n" + "="*80)
        logger.info("ğŸ§ª DRY-RUN æ¨¡å¼ï¼šä¸ä¿å­˜æ–‡ä»¶")
        logger.info(f"ğŸ“Š å·²æ›´æ–° {updated_count} ä¸ªæœŸåˆŠçš„æ©™è‰²ç³»æŒ‡æ ‡")
        logger.info("="*80)
    elif updated_count == 0:
        logger.info("\n" + "="*80)
        logger.info("â„¹ï¸ æ²¡æœ‰æ•°æ®æ›´æ–°ï¼Œè·³è¿‡ä¿å­˜")
        logger.info("="*80)
    else:
        try:
            # è½¬æ¢å›åˆ—è¡¨
            updated_jrank_data = list(jrank_dict.values())
            
            with open(jrank_file, 'w', encoding='utf-8') as f:
                yaml.dump(updated_jrank_data, f, default_flow_style=False, allow_unicode=True)
            
            logger.info("\n" + "="*80)
            logger.info(f"âœ… æˆåŠŸæ›´æ–° {jrank_file}")
            logger.info(f"ğŸ“Š å·²æ›´æ–° {updated_count} ä¸ªæœŸåˆŠçš„æ©™è‰²ç³»æŒ‡æ ‡")
            logger.info("="*80)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ›´æ–°æœŸåˆŠæ©™è‰²ç³»æŒ‡æ ‡ (æ©™è‰²åˆ†æ•°, æ©™è‰²åˆ†åŒº, Documents Published, Percentile)')
    parser.add_argument('--dry-run', '-n', action='store_true', 
                       help='æµ‹è¯•æ¨¡å¼ - ä¸ä¿å­˜æ–‡ä»¶')
    args = parser.parse_args()
    
    logger.info("="*80)
    logger.info("æœŸåˆŠæ©™è‰²ç³»æŒ‡æ ‡æ›´æ–°å™¨ (DrissionPage)")
    logger.info("="*80)
    
    try:
        update_scopus_metrics_in_yaml(dry_run=args.dry_run)
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()
