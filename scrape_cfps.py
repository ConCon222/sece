import re
import time
import os  # æ–°å¢
import yaml
from datetime import datetime, timedelta # æ–°å¢
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# === æ ¸å¿ƒåº“ ===
from curl_cffi import requests  # å¿«ï¼šç”¨äº Elsevier, Springer, Cambridge Core
from DrissionPage import ChromiumPage, ChromiumOptions  # ç¨³ï¼šç”¨äº Taylor & Francis, Wiley, SAGE

# ==========================================
# âš™ï¸ é…ç½®åŒºåŸŸ
# ==========================================
JOURNALS = [
    {
        "name": "International Journal of Educational Technology in Higher Education",
        "url": "https://link.springer.com/journal/41239/collections?filter=Open",
        "tag": ["educational technology", "higher education"]
    },
    {
        "name": "Educational Psychologist",
        "url": "https://www.tandfonline.com/journals/hedp20",
        "tag": ["educational psychology"]
    },
    {
        "name": "Educational Research Review",
        "url": "https://www.sciencedirect.com/journal/educational-research-review/about/call-for-papers",
        "tag": ["review", "general education"]
    },
    {
        "name": "Computers & Education",
        "url": "https://www.sciencedirect.com/journal/computers-and-education/about/call-for-papers",
        "tag": ["educational technology"]
    },
    {
        "name": "Studies in Science Education",
        "url": "https://www.tandfonline.com/journals/rsse20",
        "tag": ["review"]
    },
    {
        "name": "British Journal of Educational Technology",
        "url": "https://bera-journals.onlinelibrary.wiley.com/hub/journal/14678535/bjet_special_issues.htm",
        "tag": ["educational technology"]
    },
    {
        "name": "International Journal of STEM Education",
        "url": "https://link.springer.com/journal/40594/collections?filter=Open",
        "tag": ["educational technology", "STEM education"]
    },
    {
        "name": "Review of Educational Research",
        "url": "https://journals.sagepub.com/home/rer",
        "tag": ["review", "general education"]
    },
    {
        "name": "International Journal of Management Education",
        "url": "https://www.sciencedirect.com/journal/the-international-journal-of-management-education/about/call-for-papers",
        "tag": ["educational management", "higher education"]
    },
    {
        "name": "The Internet and Higher Education",
        "url": "https://www.sciencedirect.com/journal/the-internet-and-higher-education/about/call-for-papers",
        "tag": ["higher education", "educational technology"]
    },
    {
        "name": "Computer Assisted Language Learning",
        "url": "https://www.tandfonline.com/journals/ncal20",
        "tag": ["language learning", "educational technology"]
    },
    {
        "name": "Educational Technology & Society",
        "url": "https://www.j-ets.net",
        "tag": ["educational technology"]
    },
    {
        "name": "ReCALL",
        "url": "https://www.cambridge.org/core/journals/recall/announcements/call-for-papers",
        "tag": ["language learning", "educational technology"]
    },
    {
        "name": "International Journal of Computer-Supported Collaborative Learning",
        "url": "https://link.springer.com/journal/11412/collections?filter=Open",
        "tag": ["educational technology"]
    },
    {
        "name": "System",
        "url": "https://www.sciencedirect.com/journal/system/about/call-for-papers",
        "tag": ["language learning", "educational technology"]
    },
    {
        "name": "Assessing Writing",
        "url": "https://www.sciencedirect.com/journal/assessing-writing/about/call-for-papers",
        "tag": ["language learning"]
    },
    {
        "name": "Journal of Science Education and Technology",
        "url": "https://link.springer.com/journal/10956/collections?filter=Open",
        "tag": ["educational technology", "STEM education"]
    },
    {
        "name": "Education and Information Technologies",
        "url": "https://link.springer.com/journal/10639/collections?filter=Open",
        "tag": ["educational technology"]
    },
    {
        "name": "Interactive Learning Environments",
        "url": "https://www.tandfonline.com/journals/nile20",
        "tag": ["educational technology"]
    },
    {
        "name": "Academy of Management Learning & Education",
        "url": "https://journals.aom.org/journal/amle",
        "tag": ["educational management", "higher education"]
    },
    {
        "name": "Language Teaching",
        "url": "https://www.cambridge.org/core/journals/language-teaching/announcements/call-for-papers",
        "tag": ["language learning"]
    },
    {
        "name": "Journal of Research on Technology in Education",
        "url": "https://www.tandfonline.com/journals/ujrt20",
        "tag": ["educational technology"]
    },
    {
        "name": "Innovations in Education and Teaching International",
        "url": "https://www.tandfonline.com/journals/riie20",
        "tag": ["general education", "higher education"]
    },
    {
        "name": "Journal of Computing in Higher Education",
        "url": "https://www.springer.com/journal/12528/collections?filter=Open",
        "tag": ["higher education", "educational technology"]
    },
    {
        "name": "Educational Researcher",
        "url": "https://journals.sagepub.com/home/edr",
        "tag": ["general education", "educational policy"]
    },
    {
        "name": "Journal of Educational Computing Research",
        "url": "https://journals.sagepub.com/home/jec",
        "tag": ["educational technology"]
    },
    {
        "name": "Learning and Instruction",
        "url": "https://www.sciencedirect.com/journal/learning-and-instruction/about/call-for-papers",
        "tag": ["general education"]
    },
    {
        "name": "IEEE Transactions on Learning Technologies",
        "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4620076",
        "tag": ["educational technology"]
    },
    {
        "name": "Metacognition and Learning",
        "url": "https://www.springer.com/journal/11409/collections?filter=Open",
        "tag": ["educational psychology", "educational technology"]
    },
    {
        "name": "Journal of Legal Education",
        "url": "https://jle.aals.org/",
        "tag": ["educational policy", "legal education"]
    },
    {
        "name": "Asia-Pacific Education Researcher",
        "url": "https://www.springer.com/journal/40299/collections?filter=Open",
        "tag": ["general education", "educational policy"]
    },
    {
        "name": "Innovation in Language Learning and Teaching",
        "url": "https://www.tandfonline.com/journals/rill20",
        "tag": ["language learning", "educational technology"]
    },
    {
        "name": "Journal of Computer Assisted Learning",
        "url": "https://onlinelibrary.wiley.com/page/journal/13652729/homepage/call-for-papers",
        "tag": ["educational technology"]
    }
]

# ä¿®æ”¹ï¼šé€‚é… al-folio ç›®å½•ç»“æ„ï¼Œè‡ªåŠ¨å­˜å…¥ _data æ–‡ä»¶å¤¹
# è¯·ç¡®ä¿ä½ çš„é¡¹ç›®æ ¹ç›®å½•ä¸‹æœ‰ _data æ–‡ä»¶å¤¹ï¼Œæˆ–è€…è„šæœ¬ä¼šè‡ªåŠ¨åˆ›å»º
OUTPUT_YML_PATH = "_data/cfps.yml"

# ==========================================
# æœˆä»½æ˜ å°„
# ==========================================
MONTH_MAP = {
    'jan': 1, 'january': 1, 'feb': 2, 'february': 2,
    'mar': 3, 'march': 3, 'apr': 4, 'april': 4, 'may': 5,
    'jun': 6, 'june': 6, 'jul': 7, 'july': 7, 'aug': 8, 'august': 8,
    'sep': 9, 'sept': 9, 'september': 9, 'oct': 10, 'october': 10,
    'nov': 11, 'november': 11, 'dec': 12, 'december': 12,
}


class JournalCFPScraper:
    def __init__(self):
        self.date_pattern = re.compile(
            r"(\d{1,2})(?:st|nd|rd|th)?\s*"
            r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\s+"
            r"(\d{4})|"
            r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\s+"
            r"(\d{1,2})(?:st|nd|rd|th)?,?\s+(\d{4})|"
            r"(\d{4})-(\d{2})-(\d{2})|"
            r"(\d{1,2})(?:st|nd|rd|th)?\s+(\w+)\s+(\d{4})",
            re.I,
        )

        print("âš™ï¸ åˆå§‹åŒ–æµè§ˆå™¨ç»„ä»¶ (æ— å¤´æ¨¡å¼)...")
        co = ChromiumOptions()
        co.headless(True)
        co.set_argument("--no-sandbox")
        co.set_argument("--disable-gpu")
        co.set_argument("--ignore-certificate-errors")
        co.set_user_agent(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            )
        )
        self.browser = ChromiumPage(co)

    def __del__(self):
        try:
            self.browser.quit()
        except Exception:
            pass

    # --------------------------
    # é€šç”¨å·¥å…·
    # --------------------------
    def clean_text(self, text):
        if not text:
            return "N/A"
        return re.sub(r"\s+", " ", str(text)).strip()

    def normalize_for_date_extraction(self, text):
        if not text:
            return ""
        text = re.sub(r'<[^>]+>', '', str(text))
        text = re.sub(r'(\d)(st|nd|rd|th)\b', r'\1', text, flags=re.I)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_date(self, text):
        if not text:
            return None
        normalized = self.normalize_for_date_extraction(text)
        m = self.date_pattern.search(normalized)
        if m:
            return self.clean_text(m.group(0))
        return None

    def parse_date_to_sort_key(self, date_str):
        """
        ä¿®æ”¹ï¼šå¦‚æœè§£æå¤±è´¥æˆ–ä¸ºç©ºï¼Œç›´æ¥è¿”å› '9999-99-99'
        """
        default_date = "9999-99-99"
        
        if not date_str or date_str in {"N/A", "æœªæ‰¾åˆ°æ—¥æœŸ", ""}:
            return default_date

        normalized = self.normalize_for_date_extraction(date_str)

        try:
            # 1. ISOæ ¼å¼: YYYY-MM-DD
            m = re.match(r'(\d{4})-(\d{2})-(\d{2})', normalized)
            if m:
                return f"{m.group(1)}-{m.group(2)}-{m.group(3)}"

            # 2. DD Month YYYY
            m = re.match(r'(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})', normalized)
            if m:
                day = int(m.group(1))
                month_str = m.group(2).lower()
                year = m.group(3)
                month = MONTH_MAP.get(month_str[:3], 0)
                if month:
                    return f"{year}-{month:02d}-{day:02d}"

            # 3. Month DD, YYYY
            m = re.match(r'([A-Za-z]+)\s+(\d{1,2}),?\s+(\d{4})', normalized)
            if m:
                month_str = m.group(1).lower()
                day = int(m.group(2))
                year = m.group(3)
                month = MONTH_MAP.get(month_str[:3], 0)
                if month:
                    return f"{year}-{month:02d}-{day:02d}"

            # 4. Range: å–æœ€åä¸€ä¸ªæ—¥æœŸ
            dates_found = re.findall(r'(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})', normalized)
            if dates_found:
                day, month_str, year = dates_found[-1]
                month = MONTH_MAP.get(month_str.lower()[:3], 0)
                if month:
                    return f"{year}-{month:02d}-{int(day):02d}"
        except Exception:
            pass

        return default_date

    def fetch_page_fast(self, url, timeout=30):
        try:
            print(f"ğŸš€ [HTTP] æ­£åœ¨è®¿é—®: {url}")
            resp = requests.get(
                url,
                impersonate="chrome120",
                timeout=timeout,
                headers={
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.9",
                },
            )
            if resp.status_code == 200:
                return resp.text
            print(f"âŒ çŠ¶æ€ç é”™è¯¯ {resp.status_code}")
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return None

    # --------------------------
    # Browser å·¥å…· (ä¿æŒä¸å˜)
    # --------------------------
    def try_accept_cookies(self):
        selectors = ["css:#onetrust-accept-btn-handler", "text:Accept All Cookies", "text:Accept all", "text:Accept", "text:I Agree"]
        for sel in selectors:
            try:
                ele = self.browser.ele(sel, timeout=1)
                if ele:
                    ele.click()
                    time.sleep(1)
                    break
            except Exception:
                pass

    def scroll_to_bottom(self, rounds=3, pause=1.0):
        for _ in range(rounds):
            try:
                self.browser.scroll.to_bottom()
            except Exception:
                try:
                    self.browser.run_js("window.scrollTo(0, document.body.scrollHeight);")
                except Exception:
                    pass
            time.sleep(pause)

    def get_html_browser(self, url, wait=2, scroll_rounds=2):
        print(f"ğŸŒ [Browser] GET {url}")
        self.browser.get(url)
        time.sleep(wait)
        self.try_accept_cookies()
        time.sleep(0.8)
        if scroll_rounds > 0:
            self.scroll_to_bottom(rounds=scroll_rounds, pause=0.8)
        return self.browser.html

    # ==========================================
    # è§£æå™¨éƒ¨åˆ† (ä¿æŒåŸæ ·ï¼Œä»…åšç¼©è¿›è°ƒæ•´ç¡®è®¤)
    # ==========================================
    def _extract_text_clean(self, element):
        if not element: return ""
        html_str = str(element)
        html_str = re.sub(r'<sup[^>]*>.*?</sup>', '', html_str, flags=re.I | re.DOTALL)
        temp_soup = BeautifulSoup(html_str, 'lxml')
        return self.clean_text(temp_soup.get_text(' ', strip=True))

    def _parse_wiley_dst_listing(self, soup, journal_url):
        wrap = soup.select_one("div.DST-CFP-listing-wrap")
        if not wrap: return []
        results = []
        for it in wrap.select("div.DST-CFP-listing-item"):
            a_title = it.select_one("h3 a[href]")
            if not a_title: continue
            title = self._extract_text_clean(a_title)
            link = urljoin(journal_url, a_title.get("href"))
            a_more = it.select_one("a.DST-CFP-listing-item__more[href]")
            if a_more and a_more.get("href"): link = urljoin(journal_url, a_more.get("href"))
            
            d_el = it.select_one("p.DST-CFP-listing-item__deadline")
            deadline_text = self._extract_text_clean(d_el) if d_el else ""
            dt = self.extract_date(deadline_text)
            deadline = dt or (self.clean_text(deadline_text.split(":", 1)[1]) if ":" in deadline_text else "æœªæ‰¾åˆ°æ—¥æœŸ")
            results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": "N/A", "link": link})
        return results

    def _parse_wiley_h4_blocks(self, soup, journal_url):
        results = []
        for h4 in soup.find_all("h4"):
            try:
                a_tags = h4.find_all("a", href=True)
                if not a_tags: continue
                candidates = []
                for a in a_tags:
                    t = self._extract_text_clean(a)
                    if t and len(t) >= 3: candidates.append((len(t), t, a.get("href")))
                if not candidates: continue
                candidates.sort(reverse=True, key=lambda x: x[0])
                _, title, href = candidates[0]
                link = urljoin(journal_url, href)
                
                abstract_deadline, fullpaper_deadline, editor_list = "æœªæ‰¾åˆ°æ—¥æœŸ", "æœªæ‰¾åˆ°æ—¥æœŸ", []
                for sib in h4.find_next_siblings():
                    if sib.name in {"h4", "hr"}: break
                    if sib.name == "div" and "border-top" in (sib.get("style") or "").lower(): break
                    if sib.name == "p":
                        txt = self._extract_text_clean(sib)
                        lower = txt.lower()
                        if "deadline" in lower:
                            dt = self.extract_date(txt)
                            if "abstract" in lower: abstract_deadline = dt or abstract_deadline
                            elif "full paper" in lower or "full-paper" in lower: fullpaper_deadline = dt or fullpaper_deadline
                            elif dt and fullpaper_deadline == "æœªæ‰¾åˆ°æ—¥æœŸ": fullpaper_deadline = dt
                    if sib.name == "ul":
                        editor_list = [self._extract_text_clean(li) for li in sib.find_all("li") if li.get_text(strip=True)]
                
                if title and title != "N/A":
                    results.append({"title": title, "abstract_deadline": abstract_deadline, "fullpaper_deadline": fullpaper_deadline, "editors": "; ".join(editor_list) if editor_list else "N/A", "desc": "N/A", "link": link})
            except Exception: continue
        return results

    def parse_wiley_browser(self, journal_url):
        print(f"ğŸŒ [Browser] Wiley: {journal_url}")
        try:
            html = self.get_html_browser(journal_url, wait=3, scroll_rounds=2)
            soup = BeautifulSoup(html, "lxml")
            results = self._parse_wiley_dst_listing(soup, journal_url) + self._parse_wiley_h4_blocks(soup, journal_url)
            uniq = {}
            for r in results: uniq[(r.get("title"), r.get("link"))] = r
            return list(uniq.values())
        except Exception as e:
            print(f"âŒ Wiley å¼‚å¸¸: {e}")
            return []

    def _tf_parse_detail_page_html(self, html, page_url):
        soup = BeautifulSoup(html, "lxml")
        title = "æœªçŸ¥æ ‡é¢˜"
        hero_h2 = soup.select_one("section.layout__hero h2")
        if hero_h2: title = self._extract_text_clean(hero_h2)
        else:
            h2 = soup.find("h2")
            if h2: title = self._extract_text_clean(h2)
        
        abstract_deadline, fullpaper_deadline, editors, desc = "æœªæ‰¾åˆ°æ—¥æœŸ", "æœªæ‰¾åˆ°æ—¥æœŸ", "N/A", "N/A"
        for sec in soup.select("section.layout__deadline--title"):
            val = self._extract_text_clean(sec.select_one("time"))
            label = self._extract_text_clean(sec.select_one("h3")).lower()
            dt = self.extract_date(val) or val
            if "abstract" in label: abstract_deadline = dt or abstract_deadline
            elif "manuscript" in label or "full" in label or "paper" in label: fullpaper_deadline = dt or fullpaper_deadline
        
        ed_sec = soup.select_one("section.layout__editors")
        if ed_sec:
            people = []
            for p in ed_sec.select("p"):
                name = self._extract_text_clean(p.select_one("strong"))
                aff = self._extract_text_clean(p.select_one("em"))
                if name and name != "N/A": people.append(f"{name} ({aff})" if aff and aff != "N/A" else name)
            if people: editors = "; ".join(people)
            
        about = soup.select_one("section.layout__about") or soup.select_one("main#main-content")
        if about:
            ps = [self._extract_text_clean(p) for p in about.select("p") if len(self._extract_text_clean(p)) >= 80]
            if ps: desc = max(ps, key=len)
            
        return {"title": title, "abstract_deadline": abstract_deadline, "fullpaper_deadline": fullpaper_deadline, "editors": editors, "desc": desc, "link": page_url}

    def parse_taylor_francis(self, journal_url):
        print(f"ğŸŒ [Browser] T&F: {journal_url}")
        results = []
        try:
            html = self.get_html_browser(journal_url, wait=6, scroll_rounds=1)
            soup = BeautifulSoup(html, "lxml")
            target_links = []
            cfp_container = soup.select_one(".cfpContent") or soup
            for a in cfp_container.select("a[href]"):
                if "think.taylorandfrancis.com" in a.get("href", ""): target_links.append(a.get("href"))
            
            for link_url in list(dict.fromkeys(target_links)):
                tab = self.browser.new_tab(link_url)
                try:
                    try: tab.wait.doc_loaded(timeout=12)
                    except: pass
                    time.sleep(2.5)
                    try: 
                        btn = tab.ele("css:#onetrust-accept-btn-handler", timeout=1)
                        if btn: btn.click()
                    except: pass
                    results.append(self._tf_parse_detail_page_html(tab.html, link_url))
                except: pass
                finally: tab.close()
        except Exception: pass
        uniq = {}
        for r in results: uniq[(r.get("title"), r.get("link"))] = r
        return list(uniq.values())

    def parse_sage_browser(self, journal_url):
        print(f"ğŸŒ [Browser] SAGE: {journal_url}")
        try:
            html = self.get_html_browser(journal_url, wait=2, scroll_rounds=3)
            soup = BeautifulSoup(html, "lxml")
            results = []
            for card in soup.select("div.marketing-spot"):
                title = self._extract_text_clean(card.select_one("h3.marketing-spot__title"))
                desc = self._extract_text_clean(card.select_one("div.marketing-spot__text"))
                a = card.select_one("div.marketing-spot__footer a[href]")
                link = urljoin(journal_url, a["href"]) if a else "N/A"
                if "closed" in desc.lower() or title == "N/A": continue
                
                # ç®€å•è¿‡æ»¤å¹¿å‘Š
                if any(x in title.lower() or x in desc.lower() for x in ["why publish", "reviewer resources", "discipline hubs"]): continue
                if not ("call" in title.lower() or "special issue" in title.lower() or "submit" in desc.lower()): continue

                deadline = self.extract_date(desc) or "æœªæ‰¾åˆ°æ—¥æœŸ"
                results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": desc, "link": link})
            
            uniq = {}
            for r in results: uniq[(r["title"], r["link"])] = r
            return list(uniq.values())
        except Exception as e:
            print(f"âŒ SAGE å¼‚å¸¸: {e}")
            return []

    def parse_elsevier(self, html, base_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        header = soup.find(["h2", "h3"], string=re.compile("Call for papers", re.I))
        container = header.find_next("ul", class_="sub-list") if header else soup.find("ul", class_="sub-list")
        if not container: return []
        for item in container.find_all("li"):
            try:
                h3 = item.find("h3")
                if not h3: continue
                title = self._extract_text_clean(h3.find("a"))
                link = urljoin(base_url, h3.find("a")["href"])
                desc = "N/A"
                intro = item.find("p", class_="intro")
                if intro: desc = self._extract_text_clean(intro)
                d_div = item.find(lambda t: t.name == "div" and "Submission deadline" in t.get_text())
                deadline = self._extract_text_clean(d_div.find("strong")) if d_div and d_div.find("strong") else (self._extract_text_clean(d_div) if d_div else "æœªæ‰¾åˆ°æ—¥æœŸ")
                editors = self._extract_text_clean(item.find("p", class_="summary")) if item.find("p", class_="summary") else "N/A"
                results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": editors, "desc": desc, "link": link})
            except: continue
        return results

    def parse_springer(self, html, base_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        for art in soup.find_all("article", class_="app-card-collection"):
            try:
                heading = art.find(["h2", "h3"], class_=re.compile("heading"))
                title = self._extract_text_clean(heading.find("a")) if heading else "N/A"
                link = urljoin(base_url, heading.find("a")["href"]) if heading else "N/A"
                desc = self._extract_text_clean(art.find("div", class_="app-card-collection__text"))
                deadline = "æœªæ‰¾åˆ°æ—¥æœŸ"
                for dt in art.find_all("dt"):
                    if "deadline" in dt.get_text().lower() and dt.find_next_sibling("dd"):
                        deadline = self._extract_text_clean(dt.find_next_sibling("dd"))
                        break
                if deadline != "æœªæ‰¾åˆ°æ—¥æœŸ":
                    results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": desc, "link": link})
            except: continue
        return results

    def parse_cambridge_core_call_for_papers(self, html, base_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        for ov in (soup.select_one("#maincontent") or soup).select("ul.overview.no-margin-bottom-for-small"):
            a = ov.select_one("li.title a[href]")
            if not a: continue
            title = self._extract_text_clean(a)
            link = urljoin(base_url, a["href"])
            date_el = ov.select_one("li.date")
            deadline = self._extract_text_clean(date_el) if date_el else "æœªæ‰¾åˆ°æ—¥æœŸ"
            desc = self._extract_text_clean(ov.select_one("li.description"))
            results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": desc, "link": link})
        uniq = {}
        for r in results: uniq[(r["title"], r["link"])] = r
        return list(uniq.values())

    # ==========================================
    # æ•°æ®è§„èŒƒåŒ– & è¾“å‡º (æ ¸å¿ƒé€»è¾‘ä¿®æ”¹)
    # ==========================================
    def infer_publisher(self, journal_url, journal_name=""):
        u, n = (journal_url or "").lower(), (journal_name or "").lower()
        if "wiley" in u or "wiley" in n: return "Wiley"
        if "tandf" in u or "taylor" in n: return "Taylor & Francis"
        if "sage" in u or "sage" in n: return "SAGE"
        if "sciencedirect" in u or "elsevier" in n: return "Elsevier"
        if "springer" in u or "springer" in n: return "Springer"
        if "cambridge" in u or "cambridge" in n: return "Cambridge Core"
        return "Unknown"

    def _empty_if_na(self, s):
        s = "" if s is None else str(s).strip()
        return "" if s in {"N/A", "æœªæ‰¾åˆ°æ—¥æœŸ"} else s

    def normalize_item_for_yaml(self, journal, item):
        fullpaper_deadline = self._empty_if_na(item.get("fullpaper_deadline", "") or item.get("deadline", ""))
        
        # å³ä½¿è¿™é‡Œæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œparse_date_to_sort_key ä¹Ÿä¼šè¿”å› '9999-99-99'
        fullpaper_deadline_sort = self.parse_date_to_sort_key(fullpaper_deadline)

        # è·å– raw_tagï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²å°±è½¬æˆåˆ—è¡¨ï¼Œå¦‚æœæ˜¯åˆ—è¡¨å°±ä¿æŒåŸæ ·
        raw_tag = journal.get("tag", [])
        if isinstance(raw_tag, str):
            # å¦‚æœé…ç½®çš„æ˜¯ "tag": "A"ï¼Œè½¬ä¸º ["A"]
            # å¦‚æœé…ç½®çš„æ˜¯ "tag": "A, B"ï¼Œè½¬ä¸º ["A", "B"] (å¯é€‰ï¼Œçœ‹ä½ å–œå¥½ï¼Œè¿™é‡Œç®€å•å¤„ç†)
            if raw_tag:
                tag_out = [raw_tag] 
            else:
                tag_out = []
        elif isinstance(raw_tag, list):
            # å¦‚æœé…ç½®çš„æ˜¯ "tag": ["A", "B"]ï¼Œä¿æŒåŸæ ·
            tag_out = raw_tag
        else:
            tag_out = []
        # === ä¿®æ”¹ç»“æŸ ===
        
        return {
            "journal": self._empty_if_na(journal.get("name")),
            "publisher": journal.get("publisher") or self.infer_publisher(journal.get("url"), journal.get("name")),
            "tag": tag_out,
            "title": self._empty_if_na(item.get("title")),
            "abstract_deadline": self._empty_if_na(item.get("abstract_deadline")),
            "fullpaper_deadline": fullpaper_deadline,
            "fullpaper_deadline_sort": fullpaper_deadline_sort,
            "editors": self._empty_if_na(item.get("editors")),
            "link": self._empty_if_na(item.get("link")),
            "description": self._empty_if_na(item.get("desc")),
        }

    def export_to_yaml(self, records, path):
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            yaml.safe_dump(records, f, allow_unicode=True, sort_keys=False, default_flow_style=False, width=120)

    # ==========================================
    # æ ¸å¿ƒä¿®æ”¹ï¼šåˆå¹¶æ—§æ•°æ®ä¸æ–°æ•°æ®ï¼Œå¹¶æ¸…æ´—è¿‡æœŸæ•°æ®
    # ==========================================
    def merge_and_clean_records(self, new_records, file_path):
        existing_records = []
        
        # 1. è¯»å–æ—§æ•°æ®
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    existing_records = yaml.safe_load(f) or []
                print(f"ğŸ“‚ è¯»å–åˆ°å†å²æ•°æ®: {len(existing_records)} æ¡")
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ—§ YAML å¤±è´¥: {e}")

        # 2. åˆå¹¶ï¼šä½¿ç”¨ (Title, Link) ä½œä¸ºå”¯ä¸€é”®
        # åªè¦ Title å’Œ Link ç›¸åŒï¼Œæ–°æ•°æ®ä¼šè¦†ç›–æ—§æ•°æ®ï¼ˆæ›´æ–° deadline ç­‰ä¿¡æ¯ï¼‰
        # å¦‚æœæ–°æ•°æ®è¿™æ¬¡æ²¡çˆ¬åˆ°ï¼ˆnew_records é‡Œæ²¡æœ‰ï¼‰ï¼Œæ—§æ•°æ®ä¿ç•™åœ¨ merged_map ä¸­
        merged_map = {}
        
        # å…ˆæ”¾å…¥æ—§æ•°æ®
        for item in existing_records:
            key = (item.get("title"), item.get("link"))
            merged_map[key] = item
            
        # å†æ”¾å…¥æ–°æ•°æ® (è¦†ç›–æ—§çš„)
        for item in new_records:
            key = (item.get("title"), item.get("link"))
            merged_map[key] = item

        # 3. æ¸…æ´—è¿‡æœŸæ•°æ®
        final_list = []
        today = datetime.now().date()
        expire_threshold = today - timedelta(days=10) # å…è®¸è¿‡æœŸ10å¤©
        
        print(f"ğŸ§¹ æ‰§è¡Œæ¸…ç†: å‰”é™¤ {expire_threshold} ä¹‹å‰æˆªæ­¢çš„æ¡ç›®...")

        for item in merged_map.values():
            sort_date_str = item.get("fullpaper_deadline_sort")
            
            # å¦‚æœæ˜¯ 9999-99-99ï¼Œè¯´æ˜æ˜¯â€œå¾…å®šâ€æˆ–â€œé•¿æœŸâ€ï¼Œå¿…é¡»ä¿ç•™
            if sort_date_str == '9999-99-99':
                final_list.append(item)
                continue
                
            try:
                deadline_date = datetime.strptime(sort_date_str, "%Y-%m-%d").date()
                if deadline_date >= expire_threshold:
                    final_list.append(item)
                else:
                    # print(f"   ğŸ—‘ï¸ ç§»é™¤è¿‡æœŸ: {item.get('title')} ({sort_date_str})")
                    pass
            except ValueError:
                # è§£æä¸äº†æ—¥æœŸçš„ï¼Œä¿ç•™æ¯”è¾ƒå®‰å…¨
                final_list.append(item)

        # 4. æ’åºï¼šæ—¥æœŸè¶Šæ—©è¶Šé å‰ (9999ä¼šæ’åœ¨æœ€å)
        final_list.sort(key=lambda x: x.get("fullpaper_deadline_sort") or "9999-99-99")
        
        return final_list

    def run(self, output_yml_path=OUTPUT_YML_PATH):
        new_scraped_records = []
        print("ğŸ•·ï¸ å¼€å§‹çˆ¬å–ä»»åŠ¡...")

        for journal in JOURNALS:
            j_name = journal["name"]
            j_url = journal["url"]
            data = []
            
            try:
                url_l = j_url.lower()
                if "tandfonline.com" in url_l:
                    data = self.parse_taylor_francis(j_url)
                elif "wiley.com" in url_l:
                    data = self.parse_wiley_browser(j_url)
                elif "sagepub.com" in url_l:
                    data = self.parse_sage_browser(j_url)
                elif "cambridge.org/core" in url_l:
                    html = self.fetch_page_fast(j_url)
                    if html: data = self.parse_cambridge_core_call_for_papers(html, j_url)
                else:
                    html = self.fetch_page_fast(j_url)
                    if html:
                        if "springer.com" in url_l: data = self.parse_springer(html, j_url)
                        elif "sciencedirect.com" in url_l: data = self.parse_elsevier(html, j_url)
                
                if data:
                    print(f"   âœ… {j_name}: æŠ“å–æˆåŠŸ {len(data)} æ¡")
                    for item in data:
                        rec = self.normalize_item_for_yaml(journal, item)
                        if rec["title"] or rec["link"]:
                            new_scraped_records.append(rec)
                else:
                    print(f"   âš ï¸ {j_name}: æœªæŠ“å–åˆ°æ•°æ® (å°†ä¿ç•™å†å²æ•°æ®)")

            except Exception as e:
                print(f"   âŒ {j_name} çˆ¬å–å¤±è´¥: {e} (å°†ä¿ç•™å†å²æ•°æ®)")

        # è°ƒç”¨åˆå¹¶ä¸æ¸…ç†é€»è¾‘
        final_records = self.merge_and_clean_records(new_scraped_records, output_yml_path)

        self.export_to_yaml(final_records, output_yml_path)
        print(f"ğŸ‰ å¤„ç†å®Œæˆ! æœ€ç»ˆå†™å…¥: {output_yml_path} / æ€»è®°å½•æ•°: {len(final_records)}")

if __name__ == "__main__":
    scraper = JournalCFPScraper()
    scraper.run()