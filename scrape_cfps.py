import re
import time
import os
import yaml
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# === æ ¸å¿ƒåº“ ===
# å¦‚æœæŠ¥é”™ no module named 'curl_cffi'ï¼Œè¯· pip install curl_cffi
from curl_cffi import requests

# ==========================================
# âš™ï¸ é…ç½®åŒºåŸŸ
# ==========================================
OUTPUT_YML_PATH = "_data/cfps.yml"

JOURNALS = [
    # ... (ä¿æŒä½ åŸæœ‰çš„æœŸåˆŠåˆ—è¡¨ä¸å˜ï¼Œè¿™é‡Œä¸ºäº†èŠ‚çœç¯‡å¹…ç•¥å»ï¼Œç›´æ¥å¤åˆ¶ä½ åŸæ¥çš„åˆ—è¡¨å³å¯) ...
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘æ”¾å‡ ä¸ªå…³é”®çš„ä¾‹å­ï¼Œè¯·åŠ¡å¿…æŠŠä½ å®Œæ•´çš„ JOURNALS åˆ—è¡¨è´´å›æ¥ï¼
    {
        "name": "International Journal of Educational Technology in Higher Education",
        "url": "https://link.springer.com/journal/41239/collections?filter=Open",
        "tag": ["educational technology", "higher education"]
    },
    {
        "name": "Computers & Education",
        "url": "https://www.sciencedirect.com/journal/computers-and-education/about/call-for-papers",
        "tag": ["educational technology"]
    },
    {
        "name": "British Journal of Educational Technology",
        "url": "https://bera-journals.onlinelibrary.wiley.com/hub/journal/14678535/bjet_special_issues.htm",
        "tag": ["educational technology"]
    },
    {
        "name": "Review of Educational Research",
        "url": "https://journals.sagepub.com/home/rer",
        "tag": ["review", "general education"]
    },
    {
        "name": "Educational Psychologist",
        "url": "https://www.tandfonline.com/journals/hedp20",
        "tag": ["educational psychology"]
    },
     # ... è¯·ç¡®ä¿æŠŠæ‰€æœ‰æœŸåˆŠåˆ—è¡¨ç²˜è´´å›è¿™é‡Œ ...
]

# å‡è®¾ä½ å·²ç»æŠŠå®Œæ•´çš„ JOURNALS åˆ—è¡¨å¡«åœ¨ä¸Šé¢äº†ï¼Œä¸‹é¢æ˜¯é€»è¾‘éƒ¨åˆ†

# ==========================================
# æœˆä»½æ˜ å°„
# ==========================================
MONTH_MAP = {
    'jan': 1, 'january': 1, 'feb': 2, 'february': 2,
    'mar': 3, 'march': 3, 'apr': 4, 'april': 4, 'may': 5,
    'jun': 6, 'june': 6, 'jul': 7, 'july': 7, 'aug': 8, 'august': 8,
    'sep': 9, 'sept': 9, 'september': 9, 'oct': 10, 'october': 10,
    'nov': 11, 'november': 11, 'dec': 12, 'december': 12,
}

class JournalCFPScraper:
    def __init__(self):
        self.date_pattern = re.compile(
            r"(\d{1,2})(?:st|nd|rd|th)?\s*"
            r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\s+"
            r"(\d{4})|"
            r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\s+"
            r"(\d{1,2})(?:st|nd|rd|th)?,?\s+(\d{4})|"
            r"(\d{4})-(\d{2})-(\d{2})|"
            r"(\d{1,2})(?:st|nd|rd|th)?\s+(\w+)\s+(\d{4})",
            re.I,
        )
        # å³ä½¿ä¸ç”¨æµè§ˆå™¨ï¼Œä¹Ÿå®šä¹‰ä¸€ä¸ª session å¯èƒ½ä¼šå¤ç”¨è¿æ¥
        self.session = requests.Session()

    # --------------------------
    # é€šç”¨å·¥å…·
    # --------------------------
    def clean_text(self, text):
        if not text:
            return "N/A"
        return re.sub(r"\s+", " ", str(text)).strip()

    def normalize_for_date_extraction(self, text):
        if not text:
            return ""
        text = re.sub(r'<[^>]+>', '', str(text))
        text = re.sub(r'(\d)(st|nd|rd|th)\b', r'\1', text, flags=re.I)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_date(self, text):
        if not text:
            return None
        normalized = self.normalize_for_date_extraction(text)
        m = self.date_pattern.search(normalized)
        if m:
            return self.clean_text(m.group(0))
        return None

    def parse_date_to_sort_key(self, date_str):
        default_date = "9999-99-99"
        if not date_str or date_str in {"N/A", "æœªæ‰¾åˆ°æ—¥æœŸ", ""}:
            return default_date
        
        normalized = self.normalize_for_date_extraction(date_str)
        try:
            m = re.match(r'(\d{4})-(\d{2})-(\d{2})', normalized)
            if m: return f"{m.group(1)}-{m.group(2)}-{m.group(3)}"

            m = re.match(r'(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})', normalized)
            if m:
                day, month_str, year = int(m.group(1)), m.group(2).lower(), m.group(3)
                month = MONTH_MAP.get(month_str[:3], 0)
                if month: return f"{year}-{month:02d}-{day:02d}"

            m = re.match(r'([A-Za-z]+)\s+(\d{1,2}),?\s+(\d{4})', normalized)
            if m:
                month_str, day, year = m.group(1).lower(), int(m.group(2)), m.group(3)
                month = MONTH_MAP.get(month_str[:3], 0)
                if month: return f"{year}-{month:02d}-{day:02d}"

            dates_found = re.findall(r'(\d{1,2})\s+([A-Za-z]+)\s+(\d{4})', normalized)
            if dates_found:
                day, month_str, year = dates_found[-1]
                month = MONTH_MAP.get(month_str.lower()[:3], 0)
                if month: return f"{year}-{month:02d}-{int(day):02d}"
        except Exception:
            pass
        return default_date

    def fetch_page(self, url, timeout=30):
        """
        ç»Ÿä¸€ä½¿ç”¨ curl_cffi è·å–é¡µé¢ã€‚
        """
        try:
            print(f"ğŸš€ [HTTP] æ­£åœ¨è®¿é—®: {url}")
            # éšæœºå»¶è¿Ÿï¼Œå‡å°‘å¹¶å‘è§¦å‘WAFçš„æ¦‚ç‡
            time.sleep(random.uniform(1, 3))
            
            resp = self.session.get(
                url,
                impersonate="chrome110",  # å°è¯• chrome110ï¼Œæœ‰æ—¶æ¯”æœ€æ–°ç‰ˆæ›´ç¨³å®š
                timeout=timeout,
                headers={
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.9",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                    "Referer": "https://www.google.com/" 
                },
                allow_redirects=True
            )
            if resp.status_code in [200, 301, 302]:
                return resp.text
            elif resp.status_code == 403:
                print(f"âŒ 403 Forbidden: è¢« WAF æ‹¦æˆª (Cloudflare/Akamai)")
            elif resp.status_code == 500:
                print(f"âŒ 500 Server Error")
            else:
                print(f"âŒ çŠ¶æ€ç é”™è¯¯ {resp.status_code}")
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return None

    def _extract_text_clean(self, element):
        if not element: return ""
        html_str = str(element)
        html_str = re.sub(r'<sup[^>]*>.*?</sup>', '', html_str, flags=re.I | re.DOTALL)
        temp_soup = BeautifulSoup(html_str, 'lxml')
        return self.clean_text(temp_soup.get_text(' ', strip=True))

    # ==========================================
    # é’ˆå¯¹ä¸åŒå‡ºç‰ˆç¤¾çš„é™æ€è§£æå™¨ (å…¨éƒ¨å»é™¤äº† Browser ä¾èµ–)
    # ==========================================

    def parse_wiley_static(self, html, journal_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        
        # æ¨¡å¼1: DST-CFP-listing-wrap (æ–°ç‰ˆé¡µé¢)
        wrap = soup.select_one("div.DST-CFP-listing-wrap")
        if wrap:
            for it in wrap.select("div.DST-CFP-listing-item"):
                a_title = it.select_one("h3 a[href]")
                if not a_title: continue
                title = self._extract_text_clean(a_title)
                link = urljoin(journal_url, a_title.get("href"))
                d_el = it.select_one("p.DST-CFP-listing-item__deadline")
                deadline_text = self._extract_text_clean(d_el) if d_el else ""
                dt = self.extract_date(deadline_text)
                deadline = dt or (self.clean_text(deadline_text.split(":", 1)[1]) if ":" in deadline_text else "æœªæ‰¾åˆ°æ—¥æœŸ")
                results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": "N/A", "link": link})

        # æ¨¡å¼2: ä¼ ç»Ÿçš„ h4/h3 æ ‡é¢˜å— (æ—§ç‰ˆé¡µé¢)
        for h_tag in soup.find_all(["h3", "h4"]):
            try:
                a_tag = h_tag.find("a", href=True)
                if not a_tag: continue
                title = self._extract_text_clean(a_tag)
                if len(title) < 5: continue 
                
                link = urljoin(journal_url, a_tag.get("href"))
                deadline = "æœªæ‰¾åˆ°æ—¥æœŸ"
                
                # å‘ä¸‹æŸ¥æ‰¾ç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜
                for sib in h_tag.find_next_siblings():
                    if sib.name in ["h3", "h4", "hr", "section"]: break
                    text = self._extract_text_clean(sib).lower()
                    if "deadline" in text:
                        dt = self.extract_date(text)
                        if dt: 
                            deadline = dt
                            break
                
                if title != "N/A":
                    results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": "N/A", "link": link})
            except: continue
            
        return results

    def parse_taylor_francis_static(self, html, journal_url):
        # T&F é™æ€æŠ“å–å¾ˆéš¾ï¼Œå› ä¸ºå†…å®¹å¾€å¾€åœ¨ iframe æˆ– JS é‡Œã€‚
        # ä½†æˆ‘ä»¬å°è¯•æŠ“å–ä¸»é¡µä¸Šçš„ "Call for papers" é“¾æ¥åŒºåŸŸ
        soup = BeautifulSoup(html, "lxml")
        results = []
        
        # å¯»æ‰¾åŒ…å« "Call for papers" çš„é“¾æ¥
        # T&F å¸¸è§ç»“æ„: <a href="...">Call for papers</a>
        candidates = []
        for a in soup.find_all("a", href=True):
            if "call for paper" in a.get_text().lower():
                candidates.append(urljoin(journal_url, a['href']))
        
        # å¦‚æœæ‰¾åˆ°äº†å…·ä½“çš„ CFP åˆ—è¡¨é¡µï¼Œéœ€è¦å†è¯·æ±‚ä¸€æ¬¡é‚£ä¸ªåˆ—è¡¨é¡µ
        # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªè®°å½•æ‰¾åˆ°äº† "Call for Papers" çš„å…¥å£ï¼Œæˆ–è€…å°è¯•è§£æå½“å‰é¡µ
        # å¦‚æœå½“å‰é¡µå°±æ˜¯åˆ—è¡¨é¡µ (é€šå¸¸ URL åŒ…å« calls-for-papers)
        
        container = soup.select_one(".cfpContent") # T&F æŸäº›é¡µé¢çš„å®¹å™¨
        if container:
             for a in container.select("a[href]"):
                link = urljoin(journal_url, a.get("href"))
                title = self._extract_text_clean(a)
                if len(title) > 10:
                    results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "editors": "N/A", "desc": "T&F Link", "link": link})
        
        # ç®€å•çš„å…œåº•ï¼šå¦‚æœæ²¡è§£æå‡ºå…·ä½“æ¡ç›®ï¼Œä½†æ²¡æŠ¥é”™ï¼Œå°±ä¸è¿”å›æ•°æ®ï¼ˆä¿ç•™å†å²ï¼‰
        return results

    def parse_sage_static(self, html, journal_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        # SAGE Marketing Spots
        for card in soup.select("div.marketing-spot"):
            title = self._extract_text_clean(card.select_one("h3.marketing-spot__title"))
            desc = self._extract_text_clean(card.select_one("div.marketing-spot__text"))
            a = card.select_one("div.marketing-spot__footer a[href]")
            link = urljoin(journal_url, a["href"]) if a else "N/A"
            
            if "closed" in desc.lower() or title == "N/A": continue
            if any(x in title.lower() for x in ["why publish", "reviewer resources"]): continue
            if not ("call" in title.lower() or "special issue" in title.lower() or "submit" in desc.lower()): continue

            deadline = self.extract_date(desc) or "æœªæ‰¾åˆ°æ—¥æœŸ"
            results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": desc, "link": link})
        return results

    def parse_elsevier(self, html, base_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        # Elsevier ç»“æ„ç»å¸¸å˜ï¼Œè¿™é‡Œä¿ç•™åŸæœ¬é€»è¾‘
        header = soup.find(["h2", "h3"], string=re.compile("Call for papers", re.I))
        container = header.find_next("ul", class_="sub-list") if header else soup.find("ul", class_="sub-list")
        if not container: return []
        for item in container.find_all("li"):
            try:
                h3 = item.find("h3")
                if not h3: continue
                title = self._extract_text_clean(h3.find("a"))
                link = urljoin(base_url, h3.find("a")["href"])
                d_div = item.find(lambda t: t.name == "div" and "Submission deadline" in t.get_text())
                deadline = self._extract_text_clean(d_div.find("strong")) if d_div and d_div.find("strong") else "æœªæ‰¾åˆ°æ—¥æœŸ"
                results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": "N/A", "link": link})
            except: continue
        return results

    def parse_springer(self, html, base_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        for art in soup.find_all("article", class_="app-card-collection"):
            try:
                heading = art.find(["h2", "h3"], class_=re.compile("heading"))
                title = self._extract_text_clean(heading.find("a")) if heading else "N/A"
                link = urljoin(base_url, heading.find("a")["href"]) if heading else "N/A"
                deadline = "æœªæ‰¾åˆ°æ—¥æœŸ"
                for dt in art.find_all("dt"):
                    if "deadline" in dt.get_text().lower() and dt.find_next_sibling("dd"):
                        deadline = self._extract_text_clean(dt.find_next_sibling("dd"))
                        break
                if deadline != "æœªæ‰¾åˆ°æ—¥æœŸ":
                    results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": "N/A", "link": link})
            except: continue
        return results

    def parse_cambridge(self, html, base_url):
        soup = BeautifulSoup(html, "lxml")
        results = []
        for ov in (soup.select_one("#maincontent") or soup).select("ul.overview"):
            a = ov.select_one("li.title a[href]")
            if not a: continue
            title = self._extract_text_clean(a)
            link = urljoin(base_url, a["href"])
            date_el = ov.select_one("li.date")
            deadline = self._extract_text_clean(date_el) if date_el else "æœªæ‰¾åˆ°æ—¥æœŸ"
            results.append({"title": title, "abstract_deadline": "æœªæ‰¾åˆ°æ—¥æœŸ", "fullpaper_deadline": deadline, "editors": "N/A", "desc": "N/A", "link": link})
        return results

    # ==========================================
    # æ•°æ®è¾“å‡ºä¸åˆå¹¶
    # ==========================================
    def infer_publisher(self, url, name):
        u, n = (url or "").lower(), (name or "").lower()
        if "wiley" in u: return "Wiley"
        if "tandf" in u: return "Taylor & Francis"
        if "sage" in u: return "SAGE"
        if "sciencedirect" in u: return "Elsevier"
        if "springer" in u: return "Springer"
        if "cambridge" in u: return "Cambridge Core"
        return "Unknown"

    def normalize_item_for_yaml(self, journal, item):
        fullpaper_deadline = item.get("fullpaper_deadline", "")
        fullpaper_deadline_sort = self.parse_date_to_sort_key(fullpaper_deadline)
        
        raw_tag = journal.get("tag", [])
        tag_out = [raw_tag] if isinstance(raw_tag, str) else raw_tag

        return {
            "journal": journal.get("name"),
            "publisher": self.infer_publisher(journal.get("url"), journal.get("name")),
            "tag": tag_out,
            "title": item.get("title"),
            "abstract_deadline": item.get("abstract_deadline", ""),
            "fullpaper_deadline": fullpaper_deadline,
            "fullpaper_deadline_sort": fullpaper_deadline_sort,
            "editors": item.get("editors", "N/A"),
            "link": item.get("link"),
            "description": item.get("desc", "N/A"),
        }

    def merge_and_clean_records(self, new_records, file_path):
        existing_records = []
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    existing_records = yaml.safe_load(f) or []
            except Exception: pass

        merged_map = {}
        # å…ˆè½½å…¥æ—§æ•°æ®
        for item in existing_records:
            key = (item.get("title"), item.get("link"))
            merged_map[key] = item
        # æ–°æ•°æ®è¦†ç›–æ—§æ•°æ®
        for item in new_records:
            key = (item.get("title"), item.get("link"))
            merged_map[key] = item

        final_list = []
        today = datetime.now().date()
        expire_threshold = today - timedelta(days=10) # è¿‡æœŸ10å¤©ä¸æ˜¾ç¤º

        for item in merged_map.values():
            sort_date_str = item.get("fullpaper_deadline_sort")
            if sort_date_str == '9999-99-99':
                final_list.append(item)
                continue
            try:
                deadline_date = datetime.strptime(sort_date_str, "%Y-%m-%d").date()
                if deadline_date >= expire_threshold:
                    final_list.append(item)
            except ValueError:
                final_list.append(item)

        final_list.sort(key=lambda x: x.get("fullpaper_deadline_sort") or "9999-99-99")
        return final_list

    def run(self, output_yml_path=OUTPUT_YML_PATH):
        new_scraped_records = []
        print("ğŸ•·ï¸ å¼€å§‹çˆ¬å–ä»»åŠ¡ (Pure Requests Mode)...")

        for journal in JOURNALS:
            j_name = journal["name"]
            j_url = journal["url"]
            data = []
            
            try:
                html = self.fetch_page(j_url)
                if html:
                    url_l = j_url.lower()
                    if "tandfonline.com" in url_l:
                        data = self.parse_taylor_francis_static(html, j_url)
                    elif "wiley.com" in url_l or "onlinelibrary.wiley" in url_l:
                        data = self.parse_wiley_static(html, j_url)
                    elif "sagepub.com" in url_l:
                        data = self.parse_sage_static(html, j_url)
                    elif "cambridge.org" in url_l:
                        data = self.parse_cambridge(html, j_url)
                    elif "springer.com" in url_l:
                        data = self.parse_springer(html, j_url)
                    elif "sciencedirect.com" in url_l:
                        data = self.parse_elsevier(html, j_url)
                    else:
                        print(f"   âš ï¸ æœªçŸ¥å‡ºç‰ˆç¤¾ï¼Œè·³è¿‡: {j_name}")

                    if data:
                        print(f"   âœ… {j_name}: æŠ“å–æˆåŠŸ {len(data)} æ¡")
                        for item in data:
                            rec = self.normalize_item_for_yaml(journal, item)
                            if rec["title"] and rec["link"]:
                                new_scraped_records.append(rec)
                    else:
                        print(f"   âš ï¸ {j_name}: è§£æç»“æœä¸ºç©º (å†…å®¹å¯èƒ½è¢« JS æ¸²æŸ“æˆ–æ— æ–°CFP)")
                else:
                    print(f"   âŒ {j_name}: æ— æ³•è·å–é¡µé¢å†…å®¹")

            except Exception as e:
                print(f"   âŒ {j_name} å¤„ç†å¼‚å¸¸: {e}")

        final_records = self.merge_and_clean_records(new_scraped_records, output_yml_path)
        
        os.makedirs(os.path.dirname(output_yml_path), exist_ok=True)
        with open(output_yml_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(final_records, f, allow_unicode=True, sort_keys=False, default_flow_style=False, width=120)
        
        print(f"ğŸ‰ å¤„ç†å®Œæˆ! æœ€ç»ˆå†™å…¥: {output_yml_path} / æ€»è®°å½•æ•°: {len(final_records)}")

if __name__ == "__main__":
    scraper = JournalCFPScraper()
    scraper.run()